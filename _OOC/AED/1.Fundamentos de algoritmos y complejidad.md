# Tema 0: Qué es un algoritmo y qué es complejidad (complexity)

### 2.1 Idea intuitiva

- Un **algoritmo (algorithm)** es simplemente:
    
    > Un conjunto finito de pasos bien definidos que transforman una entrada (input) en una salida (output) y siempre terminan.
    
- No basta con “un programa que funciona”: en AED te importa **cómo de bien escala** cuando el input crece, no solo si devuelve el resultado.
    

Ejemplo cotidiano:

- Ordenar tus notas de clase a mano:
    
    - Coges una hoja, buscas la que va “antes”, la colocas, repites…  
        Eso es básicamente un algoritmo tipo **selection sort** hecho a mano.
        

Lo que nos interesa en AED:

- Si tienes 10 elementos, casi cualquier algoritmo vale.
    
- Si tienes 10 millones, solo sobreviven los algoritmos con buena **time complexity**.
    

---

### 2.2 Complejidad de tiempo y de espacio

**Complejidad temporal (time complexity)**

- Mide **cómo crece el tiempo de ejecución** de un algoritmo según crece el tamaño de la entrada `n` (input size).
    
- Se expresa típicamente como una función `T(n)`, pero no en segundos, sino en “orden de crecimiento” usando notación asintótica: **Big-O** (O-notation), Big-Ω (Omega), Big-Θ (Theta).[GeeksforGeeks+2datacamp.com+2](https://www.geeksforgeeks.org/dsa/analysis-algorithms-big-o-analysis/?utm_source=chatgpt.com)
    

**Complejidad espacial (space complexity)**

- Mide cuánta **memoria extra** usa el algoritmo además del input (auxiliary space).
    
- Por ejemplo, merge sort usa espacio extra O(n), mientras que quicksort in-place usa O(log n) espacio adicional en la pila de recursión.
    

---

### 2.3 Notación Big-O (O-notation) – idea intuitiva

**Idea intuitiva:**  
Big-O te dice “cómo de rápido empeora” un algoritmo **en el peor caso (worst case)** cuando `n` crece mucho.  
Formalmente, Big-O da un **cota superior (upper bound)** del tiempo o espacio de un algoritmo.[Wikipedia+2GeeksforGeeks+2](https://en.wikipedia.org/wiki/Big_O_notation?utm_source=chatgpt.com)

Ejemplo muy típico de clases y tutoriales:

- **O(1) – tiempo constante (constant time)**
    
    - Independiente del tamaño de la entrada.
        
    - Ejemplo: acceder a `array[i]` en un array normal.
        
- **O(n) – tiempo lineal (linear time)**
    
    - Si duplicas `n`, duplicas aproximadamente el tiempo.
        
    - Ejemplo: recorrer un array con un `for` simple.
        
- **O(n²) – tiempo cuadrático (quadratic time)**
    
    - Si duplicas `n`, el tiempo se multiplica por ~4.
        
    - Ejemplo: dos bucles anidados (nested loops) hasta `n`.
        
- **O(log n) – tiempo logarítmico (logarithmic time)**
    
    - Si duplicas `n`, el tiempo solo aumenta un poquito.
        
    - Ejemplo clásico: **binary search (búsqueda binaria)** sobre array ordenado.
        

Estos órdenes de crecimiento son estándar en cualquier curso de DSA.[neetcode.io+1](https://neetcode.io/courses/lessons/big-o-notation?utm_source=chatgpt.com)

---

### 2.4 Ejemplo concreto: búsqueda lineal vs búsqueda binaria

#### 2.4.1 Linear search (búsqueda lineal) – O(n)

Problema:  
Dado un array `arr` y un valor `target`, ¿en qué índice está `target`?

**Algoritmo lineal (linear search)**:

- Recorres de izquierda a derecha hasta encontrarlo o llegar al final.
    

```Java
public class LinearSearch {
    // Busca target en el array (array no tiene por qué estar ordenado)
    // Devuelve el índice o -1 si no está.
    public static int linearSearch(int[] arr, int target) {
        // Recorremos el array una vez -> O(n) time
        for (int i = 0; i < arr.length; i++) {
            // Comparación (comparison) O(1)
            if (arr[i] == target) {
                return i; // encontrado
            }
        }
        return -1; // no encontrado
    }

    public static void main(String[] args) {
        int[] data = {4, 1, 7, 9, 2};
        int pos = linearSearch(data, 7);
        System.out.println("Posición: " + pos); // imprime 2
    }
}

```

**Complejidad:**

- En el peor caso (worst case):
    
    - `target` no está o está al final → recorres todas las `n` posiciones.
        
    - Tiempo: `T(n) ∈ O(n)`.
        
- Espacio extra: O(1) (solo unas pocas variables).
    

---

#### 2.4.2 Binary search (búsqueda binaria) – O(log n)

Requisito: el array **debe estar ordenado**.

Idea intuitiva:

- Mantienes un rango `[low, high]`.
    
- Mires la posición media `mid`.
    
- Si `arr[mid] == target`, done.
    
- Si `target < arr[mid]`, buscas en la mitad izquierda.
    
- Si `target > arr[mid]`, buscas en la mitad derecha.
    
- Cada paso **parte el problema a la mitad** → típico patrón **O(log n)**.
    

```Java
public class BinarySearch {
    // Array debe estar ORDENADO (sorted array)
    public static int binarySearch(int[] arr, int target) {
        int low = 0;
        int high = arr.length - 1;

        // Cada iteración reduce el rango a la mitad -> O(log n) time
        while (low <= high) {
            // Cálculo del índice medio (middle index)
            int mid = low + (high - low) / 2;

            if (arr[mid] == target) {
                return mid; // encontrado
            } else if (arr[mid] < target) {
                low = mid + 1; // buscar en mitad derecha (right half)
            } else {
                high = mid - 1; // buscar en mitad izquierda (left half)
            }
        }

        return -1; // no encontrado
    }

    public static void main(String[] args) {
        int[] sortedData = {1, 3, 5, 7, 9, 11, 13};
        int pos = binarySearch(sortedData, 7);
        System.out.println("Posición: " + pos); // imprime 3
    }
}
```

**Complejidad:**

- Cada vez reduces el tamaño del problema de `n` a `n/2`, luego a `n/4`, etc.
    
- Número de pasos ≈ número de veces que puedes dividir `n` entre 2 → `log₂ n`.
    
- Tiempo: `T(n) ∈ O(log n)`.[algomap.io+1](https://algomap.io/lessons/big-o-notation?utm_source=chatgpt.com)
    
- Espacio:
    
    - Versión iterativa: O(1) espacio extra.
        
    - Versión recursiva: O(log n) por la pila de llamadas (call stack).
        

---

### 2.5 “Chuleta mental” de esta lección

**Ideas clave:**

1. Un **algoritmo** es una secuencia finita de pasos bien definidos que siempre termina.
    
2. Nos importa su **eficiencia** → cómo escala con el tamaño del input `n`.
    
3. **Time complexity** se mide usualmente con **Big-O notation (O-notation)**, que da una cota superior (upper bound) del tiempo en función de `n`.[Wikipedia+2Simplilearn.com+2](https://en.wikipedia.org/wiki/Big_O_notation?utm_source=chatgpt.com)
    
4. Orden típico de complejidades (de mejor a peor):
    
    - O(1) < O(log n) < O(n) < O(n log n) < O(n²) < O(2ⁿ) < O(n!)
        
5. **Linear search** en array desordenado: O(n) tiempo, O(1) espacio.
    
6. **Binary search** en array ordenado: O(log n) tiempo, O(1) espacio (versión iterativa).

## 1. Asintótico: qué estamos midiendo realmente

En teoría de algoritmos se usan **notaciones asintóticas (asymptotic notation)** para describir el crecimiento de una función cuando `n → ∞`.[Curso CCS+1](https://course.ccs.neu.edu/cs5002f18-seattle/lects/cs5002_lect9_fall18_notes.pdf?utm_source=chatgpt.com)

- A nosotros esa función suele ser el **tiempo de ejecución** de un algoritmo: `T(n)`.
    
- No queremos contar instrucciones una por una: eso depende de CPU, compilador, lenguaje…  
    Queremos una **clase de crecimiento**: ¿es lineal, cuadrática, logarítmica…?[Ranger+1](https://ranger.uta.edu/~alex/courses/3318/lectures/02_counting_instr_O_V2_p1.pdf?utm_source=chatgpt.com)
    

La idea es:

> **No nos importa el valor exacto de `T(n)`; nos importa “qué tan rápido crece” cuando `n` es grande.**

Por eso aparecen estas tres estrellas:

- **Big-O (O-notation)** → cota superior (upper bound).
    
- **Big-Omega (Ω-notation)** → cota inferior (lower bound).
    
- **Big-Theta (Θ-notation)** → cota ajustada (tight bound).
    

---

## 2. Definiciones formales (pero digeribles)

Voy a usar la formulación estándar de teoría de algoritmos que verás en muchos apuntes de universidad.[Escola de Ciencias de Computación+2GeeksforGeeks+2](https://www.cs.auckland.ac.nz/courses/compsci220s1c/lectures/2014S1C/Part1/220-03.pdf?utm_source=chatgpt.com)

### 2.1 Big-O: cota superior (upper bound)

Sea `f(n)` una función que representa, por ejemplo, el tiempo de ejecución de tu algoritmo.  
Decimos que:

> `f(n)` es **O(g(n))**  
> si existen constantes positivas `c > 0` y `n₀ > 0` tales que  
> para todo `n ≥ n₀` se cumple
> 
> `0 ≤ f(n) ≤ c · g(n)`.

Interpretación en humano:

- A partir de cierto tamaño `n₀`, `f(n)` nunca crece más rápido que `g(n)` salvo por un factor constante `c`.
    
- `g(n)` es una **cota superior** del crecimiento de `f(n)`.
    

Ejemplo típico:  
`f(n) = 3n² + 5n + 20` es **O(n²)**:

- a partir de cierto `n`, `3n² + 5n + 20 ≤ 4n²` (por ejemplo)
    
- entonces `c = 4`, `g(n) = n²`.
    

Esto resume la idea de que el término que manda es `n²` y el resto son “detalles” a nivel asintótico.

---

### 2.2 Big-Omega: cota inferior (lower bound)

Definición análoga pero con desigualdad al revés.[Escola de Ciencias de Computación+1](https://www.cs.auckland.ac.nz/courses/compsci220s1c/lectures/2014S1C/Part1/220-03.pdf?utm_source=chatgpt.com)

> `f(n)` es **Ω(g(n))**  
> si existen constantes positivas `c > 0` y `n₀ > 0` tales que  
> para todo `n ≥ n₀` se cumple
> 
> `0 ≤ c · g(n) ≤ f(n)`.

Interpretación:

- A partir de cierto `n₀`, `f(n)` **nunca baja** de `c · g(n)`.
    
- `g(n)` es una **cota inferior** del crecimiento de `f(n)`.
    

Ejemplo:  
`f(n) = 3n² + 5n + 20` es **Ω(n²)** porque para `n` suficientemente grande se cumple  
`3n² + 5n + 20 ≥ 3n²`, y `3n² = c · n²` con `c = 3`.

---

### 2.3 Big-Theta: cota ajustada (tight bound)

Aquí combinamos ambas ideas.[Escola de Ciencias de Computación+2dsc40b.com+2](https://www.cs.auckland.ac.nz/courses/compsci220s1c/lectures/2014S1C/Part1/220-03.pdf?utm_source=chatgpt.com)

> `f(n)` es **Θ(g(n))**  
> si existen constantes positivas `c₁, c₂` y `n₀` tales que  
> para todo `n ≥ n₀` se cumple:
> 
> `c₁ · g(n) ≤ f(n) ≤ c₂ · g(n)`.

Es decir:

- `g(n)` es a la vez cota superior (Big-O) y cota inferior (Big-Ω) de `f(n)`.
    
- Las dos funciones **crecen al mismo ritmo** (same growth rate) asintóticamente.
    

Ejemplo clásico:

- `f(n) = 3n² + 5n + 20` es **Θ(n²)** porque:
    
    - es O(n²) (ya lo vimos),
        
    - es Ω(n²),  
        ⇒ por definición, también es Θ(n²).[Escola de Ciencias de Computación+1](https://www.cs.auckland.ac.nz/courses/compsci220s1c/lectures/2014S1C/Part1/220-03.pdf?utm_source=chatgpt.com)
        

En la práctica, cuando decimos:

> “El algoritmo es O(n²)”  
> si sabemos que esa cota es ajustada, mucha gente usa **Θ(n²)** para ser más preciso.

---

## 3. Propiedades prácticas que deberías tener en la cabeza

Hay varias reglas de cálculo que se usan constantemente para simplificar expresiones de complejidad. Las verás en cualquier guía de asintótico.[courses.cs.washington.edu+2DEV Community+2](https://courses.cs.washington.edu/courses/cse373/19su/files/lectures/slides/lecture04.pdf?utm_source=chatgpt.com)

### 3.1 Ignorar constantes multiplicativas

Si `f(n) = c · g(n)` con `c > 0` constante, entonces:

- `f(n)` es O(g(n)), Ω(g(n)) y Θ(g(n)).
    

Así que da igual si tu algoritmo tarda `2n` o `5n` operaciones → siempre decimos **Θ(n)**.

### 3.2 Dominancia de términos (dominating term)

Para polinomios, el término de mayor grado manda:

- `n² + 100n + 1000` es Θ(n²).
    
- `5n³ + 2n²` es Θ(n³).
    
- `n log n + 3n` es Θ(n log n) (porque `n log n` crece más rápido que `n`).[DEV Community+1](https://dev.to/princem/asymptotic-notations-a-comprehensive-guide-30i8?utm_source=chatgpt.com)
    

Regla mental:

> Te quedas con el término que crece más rápido cuando `n → ∞`.

### 3.3 Sumas y productos

Si tienes:

- `T(n) = f(n) + g(n)` → la complejidad es la del término más grande: `max{f, g}`.
    
- `T(n) = f(n) · g(n)` → normalmente las complejidades se multiplican.
    

Ejemplo:
```Text
for i = 1..n:       // O(n)
  for j = 1..n:     // O(n)
    constante       // O(1)
```
- El bucle interior es O(n) (j va de 1 a n).
    
- El exterior se repite n veces.
    
- Total iteraciones ~ n · n = n² → tiempo Θ(n²).[web.cs.unlv.edu+2pages.cs.wisc.edu+2](https://web.cs.unlv.edu/larmore/Courses/CSC477/S24/Handouts/nested.pdf?utm_source=chatgpt.com)
    

---

## 4. Aplicación directa a código con bucles (loops)

Voy a mezclar teoría con código Java, que es lo que luego te interesa para trabajo real.

### 4.1 Ejemplo 1: bucle simple
```Java
void example1(int[] arr) {
    int sum = 0;                 // O(1)
    for (int i = 0; i < arr.length; i++) {  // i recorre de 0 a n-1
        sum += arr[i];           // O(1) por iteración
    }
    System.out.println(sum);     // O(1)
}
```

	

Sea n = arr.length.

    La línea del for se ejecuta n veces (condición + incremento).

    El cuerpo del bucle tiene costo constante O(1).

    Total ≈ n · O(1) = O(n).
    web.cs.unlv.edu+2pages.cs.wisc.edu+2

**Conclusión:**
Tiempo **Θ(n)**, espacio extra O(1).

### 4.2 Ejemplo 2: bucles anidados (nested loops) cuadrados

```Java
void example2(int n) {
    int counter = 0;             // O(1)
    for (int i = 0; i < n; i++) {        // n iteraciones
        for (int j = 0; j < n; j++) {    // n iteraciones por cada i
            counter++;           // O(1)
        }
    }
    System.out.println(counter); // O(1)
}
```

- Bucle interior: `j = 0..n-1` → n iteraciones → O(n).
    
- Bucle exterior: `i = 0..n-1` → n iteraciones.
    
- Cada iteración del exterior dispara **n** iteraciones del interior.
    
- Total: `n · n = n²` operaciones de `counter++`.[web.cs.unlv.edu+2cs.umd.edu+2](https://web.cs.unlv.edu/larmore/Courses/CSC477/S24/Handouts/nested.pdf?utm_source=chatgpt.com)
    

**Conclusión:**  
Tiempo **Θ(n²)**, espacio extra O(1).

---

### 4.3 Ejemplo 3: bucles anidados, pero el interior depende de `i`

Este es de los que confunden a mucha gente y es muy típico en clase/entrevista.[web.cs.unlv.edu+1](https://web.cs.unlv.edu/larmore/Courses/CSC477/S24/Handouts/nested.pdf?utm_source=chatgpt.com)
```Java
void example3(int n) {
    int counter = 0;
    for (int i = 0; i < n; i++) {        // i: 0..n-1
        for (int j = 0; j < i; j++) {    // j: 0..i-1
            counter++;                   // O(1)
        }
    }
}
```

- Para cada `i`, el número de iteraciones del bucle interior es `i`.
    
- Total de iteraciones de `counter++`:
```Text
i = 0 → 0 veces
i = 1 → 1 vez
i = 2 → 2 veces
...
i = n-1 → n-1 veces
----------------------
suma = 0 + 1 + 2 + ... + (n-1)
     = (n-1)n / 2
     = Θ(n²)
```

Por tanto, **aunque el interior no está hasta `n` sino hasta `i`**, el crecimiento sigue siendo cuadrático **Θ(n²)**.

Mucha gente intuitivamente dice “es O(n·n) porque son dos bucles” y aciertan;  
pero **la razón real** es la suma `0+1+...+(n-1)` que es Θ(n²).[cs.umd.edu+1](https://www.cs.umd.edu/~meesh/351/mount/lectures/lect3-sums-and-loops.pdf?utm_source=chatgpt.com)

---

### 4.4 Ejemplo 4: bucle que reduce por mitad (logarítmico)

Ahora un bucle que no suma 1 a 1, sino que divide `i` entre 2 en cada paso: típico patrón de O(log n).
```Java
void example4(int n) {
    int x = n;
    while (x > 1) {
        x = x / 2;   // reduce x a la mitad cada vez
    }
}
```

¿Cómo lo vemos?

- `x` toma valores: `n, n/2, n/4, n/8, ..., 1`.
    
- ¿Cuántas veces puedes dividir entre 2 hasta llegar a 1? → `≈ log₂ n`.
    

Formalmente:

- Después de `k` iteraciones: `x = n / 2ᵏ`.
    
- Paramos cuando `x <= 1` ⇒ `n / 2ᵏ <= 1` ⇒ `n <= 2ᵏ` ⇒ `k >= log₂ n`.
    
- Por tanto, número de iteraciones ≈ `⌊log₂ n⌋`.
    

**Conclusión:**  
Tiempo **Θ(log n)**, espacio O(1).

Este patrón es exactamente el que aparece en **binary search (búsqueda binaria)**.

---

### 4.5 Jerarquía de complejidades que debes tener interiorizada

Orden típico de menor a mayor crecimiento:[DEV Community+1](https://dev.to/princem/asymptotic-notations-a-comprehensive-guide-30i8?utm_source=chatgpt.com)

1. **O(1)** – constante (constant).
    
2. **O(log n)** – logarítmica (logarithmic).
    
3. **O(n)** – lineal (linear).
    
4. **O(n log n)** – casi lineal (n log n), típico de buenos algoritmos de ordenación (merge sort, quicksort promedio).
    
5. **O(n²)** – cuadrática (quadratic), típico de bucles anidados “full”.
    
6. **O(n³)** – cúbica (cubic), bucles triples.
    
7. **O(2ⁿ)** – exponencial (exponential), brute force en muchos problemas de combinatoria.
    
8. **O(n!)** – factorial, desastroso para casi todo.
    

En AED serio, la frontera “aceptable” para `n` grande suele ser `O(n log n)` o, como mucho, `O(n²)` si `n` no es enorme.


# Tema 0.2 – Notación asintótica avanzada (Big-O, Big-Ω, Big-Θ, little-o, little-ω) y comparación de funciones

## 1. Repaso formal rápido: Big-O, Big-Ω, Big-Θ

Vamos a ser un poco más formales que antes, pero todavía digerible.

Sea `f(n)` el tiempo (o espacio) de tu algoritmo, y `g(n)` otra función positiva (por ejemplo `n`, `n²`, `n log n`, etc.) para `n` suficientemente grande.

### 1.1 Big-O – cota superior (upper bound)

Definición formal (versión estándar en teoría de algoritmos):

> `f(n)` es **O(g(n))** si  
> existen constantes `c > 0` y `n₀ ≥ 1` tales que  
> para todo `n ≥ n₀` se cumple
> 
> `0 ≤ f(n) ≤ c · g(n)`.

Traducción a “cerebro humano”:

- A partir de cierto tamaño de problema `n₀`, `f(n)` **no crece más rápido** que `g(n)` salvo por un factor constante `c`.
    
- `g(n)` te da una **cota superior asintótica**.
    

Ejemplo clásico:  
`f(n) = 3n² + 5n + 20`.  
Para `n` suficientemente grande, `3n² + 5n + 20 ≤ 4n²`.  
Entonces:

- `c = 4`, `g(n) = n²`, `n₀` puede ser, por ejemplo, 10.  
    Con eso ya has demostrado que `f(n) ∈ O(n²)`.
    

---

### 1.2 Big-Ω – cota inferior (lower bound)

Definición:

> `f(n)` es **Ω(g(n))** si  
> existen constantes `c > 0` y `n₀ ≥ 1` tales que  
> para todo `n ≥ n₀`:
> 
> `0 ≤ c · g(n) ≤ f(n)`.

Interpretación:

- A partir de cierto `n₀`, `f(n)` siempre está **por encima** de una constante por `g(n)`.
    
- `g(n)` es una **cota inferior asintótica**.
    

Con `f(n) = 3n² + 5n + 20`:  
Para `n` grande se cumple `3n² + 5n + 20 ≥ 3n²`.  
Con `c = 3`, tienes `f(n) ≥ 3n²` ⇒ `f(n) ∈ Ω(n²)`.

---

### 1.3 Big-Θ – cota ajustada (tight bound)

Definición:

> `f(n)` es **Θ(g(n))** si  
> existen constantes `c₁, c₂ > 0` y `n₀ ≥ 1` tales que  
> para todo `n ≥ n₀` se cumple
> 
> `c₁ · g(n) ≤ f(n) ≤ c₂ · g(n)`.

Interpretación:

- `g(n)` es a la vez **cota superior** y **cota inferior**.
    
- `f(n)` y `g(n)` **crecen al mismo ritmo** asintóticamente.
    

Con el mismo ejemplo:

- Ya vimos que `f(n) ∈ O(n²)` y `f(n) ∈ Ω(n²)`.
    
- Por definición, eso implica `f(n) ∈ Θ(n²)`.
    

Regla mental importante:

> Si el **término dominante** de una función es `n^k` (con `k > 0`), entonces la función es **Θ(n^k)**.

---

## 2. Versión con límites (limits) – intuición más matemática

Si has dado algo de cálculo / análisis (limits, L’Hôpital, etc.), hay una forma muy limpia de entender esto usando límites.

Supongamos que `f(n) > 0` y `g(n) > 0` para `n` suficientemente grande. Mira el límite:

$L = \lim_{n\to\infty} \frac{f(n)}{g(n)}.$

Entonces:

- Si `L` es un número finito y positivo (`0 < L < ∞`)  
    ⇒ `f(n)` y `g(n)` son del mismo orden → `f(n) ∈ Θ(g(n))`.
    
- Si `L = 0`  
    ⇒ `f(n)` crece más despacio → `f(n) ∈ o(g(n))`.
    
- Si `L = ∞`  
    ⇒ `f(n)` crece más rápido → `f(n) ∈ ω(g(n))`.
    

Y Big-O / Big-Ω los puedes ver como versiones “sueltas” usando limsup / liminf, pero para la intuición basta con:

- `f(n) ∈ O(g(n))` si la fracción `f(n)/g(n)` está acotada por arriba (no explota demasiado).
    
- `f(n) ∈ Ω(g(n))` si `f(n)/g(n)` no se hace pequeño (no tiende a 0).
    

No necesitas manejar esto ultra formal para AED, pero como quieres mates: es una forma compacta de recordar todas las relaciones.

---

## 3. Little-o (o-notation) y little-ω (ω-notation)

Ahora entramos en las versiones “estrictas”:

- **little-o notation (`o(g(n))`)** – “crece estrictamente más despacio que g”
    
- **little-omega (`ω(g(n))`)** – “crece estrictamente más rápido que g”
    

Estas se usan menos en programación del día a día, pero sí aparecen en teoría de algoritmos, papers y formalismos más serios, y te ayudan a pensar con precisión.

### 3.1 Little-o: o(g(n))

Definición formal:

> `f(n)` es **o(g(n))** si  
> para todo `c > 0` existe un `n₀ ≥ 1` tal que  
> para todo `n ≥ n₀`:
> 
> `0 ≤ f(n) < c · g(n)`.

Dicho de otra forma:

- No solo existe un `c` que acote a `f(n)` como en Big-O,
    
- sino que **para cualquier** constante `c`, si te vas lo bastante lejos (`n` grande), `f(n)` está por debajo de `c · g(n)`.
    

Versión con límites (mucho más intuitiva):

> `f(n) ∈ o(g(n))` ⇔  
> $\lim_{n\to\infty} \frac{f(n)}{g(n)} = 0$

Ejemplos:

1. `n ∈ o(n²)`
    
    - `f(n) = n`, `g(n) = n²`.
        
    - `f(n)/g(n) = n/n² = 1/n → 0`.
        
    - Entonces `n` crece estrictamente más despacio que `n²`.
        
2. `log n ∈ o(n)`
    
    - `f(n) = log n`, `g(n) = n`.
        
    - `log n / n → 0`.
        
    - El crecimiento lineal gana de calle.
        
3. `n log n ∈ o(n²)`
    
    - `f(n) = n log n`, `g(n) = n²`.
        
    - `f/g = (n log n) / n² = (log n) / n → 0`.
        
    - Entonces `n log n` está “estrictamente por debajo” de cualquier múltiplo de `n²`.
        

---

### 3.2 Little-omega: ω(g(n))

Es el dual de little-o.

Definición formal:

> `f(n)` es **ω(g(n))** si  
> para todo `c > 0` existe `n₀` tal que  
> para todo `n ≥ n₀`:
> 
> `0 ≤ c · g(n) < f(n)`.

Versión con límites:

> `f(n) ∈ ω(g(n))` ⇔  
> $\displaystyle \lim_{n\to\infty} \frac{f(n)}{g(n)} = ∞$

Ejemplos:

1. `n² ∈ ω(n)`
    
    - `f/g = n²/n = n → ∞`.
        
    - `n²` crece estrictamente más rápido que `n`.
        
2. `n ∈ ω(log n)`
    
    - `n / log n → ∞`.
        
    - La lineal gana por paliza a logarítmica.
        

---

### 3.3 Relación entre Big-O y little-o (y entre Big-Ω y little-ω)

- Si `f(n) ∈ o(g(n))` ⇒ automáticamente `f(n) ∈ O(g(n))`.
    
    - Little-o es “más fuerte” que Big-O.
        
    - Decir `f ∈ o(g)` es decir “no solo no crece más rápido que `g`, sino que crece estrictamente más despacio”.
        
- Si `f(n) ∈ ω(g(n))` ⇒ automáticamente `f(n) ∈ Ω(g(n))`.
    

En forma intuitiva:

- **Big-O / Big-Ω**: inclusiones amplias (≤ y ≥).
    
- **Little-o / little-ω**: inclusiones estrictas (“<<” y “>>”).
    

---

## 4. Comparar clases de funciones de forma sistemática

Vamos a construir mentalmente una “escala de crecimiento” usando estas ideas de límites.

### 4.1 Logarítmicas vs polinómicas

Hechos clave (que se pueden probar con cálculo):

1. Para cualquier `k > 0` y cualquier `ε > 0`:
    
    - `(log n)^k ∈ o(n^ε)`  
        Es decir, cualquier potencia de logaritmo crece más despacio que **cualquier** potencia de `n` por pequeña que sea.  
        En forma de límite:
        
    
				              $\lim_{n\to\infty} \frac{(\log n)^k}{n^{ε}} = 0$
1. En particular:
    
    - `log n ∈ o(n)`.
        
    - `log² n ∈ o(n)`.
        
    - `log n ∈ o(√n)`, etc.
        

Conclusión para tu intuición de AED:

> Cada vez que compares `log n` con cualquier `n^ε` (ε positivo), gana el polinomio.

---

### 4.2 Polinomios entre sí

Si tienes `n^a` y `n^b` con `a < b`:

- Entonces `n^a ∈ o(n^b)`.
    
- De nuevo, mira el cociente:
    
    $\displaystyle \frac{n^{a}}{n^{b}} = n^{a-b} = \frac{1}{n^{b-a}} \to 0$
- Así, `n` (n¹) es o(n²), y `n²` es o(n³), etc.
    

En la jerarquía:  
`n < n^{1.1} < n^{1.5} < n² < n³ < ...`

---

### 4.3 Polinomios vs exponenciales

Esto es súper importante en algoritmos:

- `n^k ∈ o(c^n)` para cualquier `k > 0` y cualquier `c > 1`.
    
- En palabras:
    
    > Toda potencia de `n` (polynomial time) crece estrictamente más despacio que cualquier exponencial `c^n`.
    

Otra vez piensa en el cociente:

$\displaystyle \frac{n^{k}}{c^{n}} \to 0 \quad (n\to\infty).$

Esto es básico para entender por qué `O(2^n)` es “impracticable” para `n` grande, mientras `O(n^3)` suele ser viable.

---

### 4.4 Exponenciales vs factorial

`n!` crece MÁS rápido que `c^n` para cualquier constante `c > 1`.

- De nuevo, se puede ver mirando el cociente `n! / c^n`, que explota a infinito.
    
- Así que `c^n ∈ o(n!)`.
    

En inglés te lo venden como:

> “factorial growth (crecimiento factorial) blows up even harder than exponential growth”.

---

### 4.5 Resumen de la jerarquía típica

Para `n → ∞`, la jerarquía de crecimiento (de más lento a más rápido) es:

1. `O(1)` – constante.
    
2. `O(log n)` – logarítmica.
    
3. `O((log n)^k)` – potencias de log.
    
4. `O(n^ε)` – `ε` pequeño.
    
5. `O(n)` – lineal.
    
6. `O(n log n)` – “casi lineal”.
    
7. `O(n²), O(n³), …` – polinomiales de grado mayor.
    
8. `O(c^n)` – exponenciales (`2^n`, `3^n`, …).
    
9. `O(n!)` – factoriales.
    

Cada nivel es **little-o** del siguiente (crece estrictamente más despacio).

---

## 5. Cómo se conecta esto con el análisis real de algoritmos

Vale, todo esto son funciones, límites, letras griegas… ¿Qué haces con ello _en la práctica_?

1. Te permite **demostrar** cosas como:
    
    - “Este algoritmo no puede ser mejor que O(n log n)” porque hay un lower bound Ω(n log n) para el problema (por ejemplo, sorting por comparaciones).
        
    - “He mejorado el algoritmo de O(n²) a O(n log n)” porque puedo acotar su T(n) de esa forma.
        
2. Te ayuda a **comparar alternativas**:
    
    - Si tienes una implementación `A` con T(n) = `1000n log n`  
        y otra `B` con T(n) = `0.01 n²`,  
        para `n` pequeño quizá `B` sea más rápida, pero asintóticamente `A` (n log n) ganará para `n` muy grande.
        
3. Te da lenguaje para **razonar sobre escalabilidad**:
    
    - En curro real, muchas veces miras: “si el número de usuarios se multiplica por 10, ¿qué pasa con el tiempo de respuesta?”.
        
    - Si tu algoritmo es **O(n²)**, multiplicar n por 10 te multiplica el tiempo por ~100.
        
    - Si es **O(n log n)**, se multiplica por ≈ 10·(log 10n / log n), que es muchísimo más dócil.
        
4. Te prepara para **recurrencias (recurrences)**:
    
    - Cuando analices algoritmos recursivos (merge sort, quicksort, divide-and-conquer en general), obtendrás ecuaciones como:  
        `T(n) = 2 T(n/2) + n`.
        
    - Resolver eso te lleva a expresiones como `T(n) = Θ(n log n)` y saber comparar `n log n` con `n²` te resulta natural gracias a todo lo de hoy.
        

---

## 6. Mini “chuleta mental” de esta lección

1. **Big-O, Big-Ω, Big-Θ** son cotas asintóticas:
    
    - O(g) → cota superior (upper bound).
        
    - Ω(g) → cota inferior (lower bound).
        
    - Θ(g) → cota ajustada (tight bound).
        
2. **Little-o / little-ω** son versiones estrictas:
    
    - `f ∈ o(g)` ⇔ `f/g → 0` (f crece estrictamente más despacio).
        
    - `f ∈ ω(g)` ⇔ `f/g → ∞` (f crece estrictamente más rápido).
        
3. Relación:
    
    - `f ∈ o(g)` ⇒ `f ∈ O(g)`.
        
    - `f ∈ ω(g)` ⇒ `f ∈ Ω(g)`.
        
    - `f ∈ Θ(g)` ⇔ `f ∈ O(g)` y `f ∈ Ω(g)`.
        
4. Escala clave:
    
    - `(log n)^k ≪ n^ε ≪ n ≪ n log n ≪ n² ≪ n³ ≪ c^n ≪ n!`  
        (“≪” aquí es una forma informal de decir “es little-o de”).
        
5. En la práctica:
    
    - Te quedas con el **término dominante**.
        
    - Ignoras constantes multiplicativas.
        
    - Usas esta jerarquía para decidir si un algoritmo es aceptable para `n` grande.