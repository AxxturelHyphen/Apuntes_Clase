# Recurrencias (recurrences) y recursión (recursion)

## Introducción a las recurrencias

Una **recurrencia (recurrence)** es una ecuación que define una secuencia de valores donde cada término se expresa en función de términos anteriores. En el contexto de algoritmos, las recurrencias son fundamentales para analizar la **complejidad temporal (time complexity)** de algoritmos recursivos. Cuando un algoritmo **divide y vencerás (divide and conquer)** descompone un problema en subproblemas más pequeños y se llama recursivamente, su tiempo de ejecución se puede modelar mediante una relación de recurrencia. Por ejemplo, el algoritmo de _búsqueda binaria (binary search)_ tiene una recurrencia que refleja que en cada llamada se resuelve la mitad del problema. Resolver una recurrencia significa encontrar una fórmula explícita o un acotamiento asintótico para el costo en función del tamaño de entrada, lo que nos permite determinar el orden de crecimiento del algoritmo.

En análisis de algoritmos, establecer la recurrencia de un algoritmo recursivo es el primer paso para derivar su complejidad. Una vez formulada la recurrencia $T(n)$ (donde $T(n)$ denota el tiempo de ejecución en el peor caso para un tamaño de entrada $n$), aplicamos métodos matemáticos para **resolver la recurrencia** y obtener el comportamiento asintótico (usualmente en notación Big O, $\Theta$, $\Omega$). Esto proporciona un medio estructurado para predecir el rendimiento: en lugar de contar cada operación manualmente, describimos recursivamente el trabajo hecho y lo resolvemos con técnicas generales. Comprender las recurrencias no solo ayuda a estimar tiempos de ejecución, sino que también es esencial para diseñar algoritmos eficientes (por ejemplo, optimizaciones como **programación dinámica (dynamic programming)** surgen de interpretar y resolver recurrencias de subproblemas repetidos).

## Conceptos básicos y definiciones

**Definición formal:** Una recurrencia es una fórmula que relaciona el término $a_n$ de una sucesión con términos anteriores $a_{n-1}, a_{n-2}, \dots$ junto con **condiciones iniciales (initial conditions)** que dan valores específicos para uno o varios primeros términos. Por ejemplo, una definición recursiva del factorial es:
$0! = 1,\; n! = n \times (n-1)!$ para $n>0$.

Si definimos $T(n)$ como el tiempo de calcular $n!$ recursivamente, obtenemos la recurrencia
$T(n) = T(n-1) + O(1)$
con
$T(0) = O(1)$.
Resolver la recurrencia significa hallar una fórmula cerrada para $T(n)$ (en este caso $T(n) = \Theta(n)$, como veremos).

---

**Linealidad y orden:** Una **recurrencia lineal (linear recurrence)** es aquella en la que $a_n$ (el término n-ésimo) se expresa como combinación lineal de un número fijo de términos anteriores más, opcionalmente, una función no recursiva. En general, tiene la forma:

$c_0(n)\,a_n + c_1(n)\,a_{n-1} + \dots + c_k(n)\,a_{n-k} = F(n),$

para $n \ge k$, donde $c_i(n)$ son coeficientes (a veces constantes) y $F(n)$ es una función conocida. El _orden_ de la recurrencia es $k$, es decir, cuántos términos previos relaciona. Si $F(n) = 0$, la recurrencia es **homogénea (homogeneous)**; si $F(n)$ no es cero, es **no homogénea (non-homogeneous)**. Por ejemplo,

$T(n) = 2T(n-1) + 3$

es lineal de orden 1 no homogénea (el término independiente $3$ impide que sea homogénea). En cambio,

$a_n = 3a_{n-1} - 2a_{n-2}$

es lineal homogénea de orden 2. La linealidad implica que $T(n)$ aparece sin exponentes ni productos consigo mismo u otras $T$, solo sumas de múltiplos de valores anteriores.

---

**Recurrencias no lineales:** Son aquellas donde el término se define de forma no lineal en términos de anteriores (por ejemplo, multiplicaciones de $T$ previos, exponentes, etc.). Un caso importante en algoritmos es la recurrencia de Fibonacci:

$F(n) = F(n-1) + F(n-2)$

(con $F(0) = 0,\; F(1) = 1$), que aunque es lineal en valores _de la sucesión_, se traduce en un costo no lineal cuando se implementa recursivamente, como veremos. Las recurrencias no lineales a menudo son más difíciles de resolver directamente y pueden requerir transformaciones ingeniosas (**cambio de variable (change of variable)**, usar series generadoras, etc.). En análisis de algoritmos, muchas recurrencias comunes resultan ser lineales (p.ej. divide y vencerás típicamente produce combinaciones lineales de subllamadas).

---

**Solución de una recurrencia:** Puede referirse a encontrar una fórmula explícita exacta para $a_n$ (solución cerrada) o, en la práctica de algoritmos, determinar la **complejidad asintótica** $T(n) = \Theta(g(n))$ para alguna función simple $g(n)$. Por ejemplo, resolver

$T(n) = 2T(n/2) + n$

da

$T(n) = \Theta(n \log n)$.

No todas las recurrencias tienen soluciones simples en forma cerrada, pero podemos casi siempre acotarlas asintóticamente con técnicas estándar. A continuación introduciremos varios métodos generales para resolver recurrencias o aproximar su orden de crecimiento: el **método de sustitución (substitution method)**, el **método del árbol de recursión (recursion tree)**, el **teorema maestro (Master Theorem)** y la **sustitución iterativa (backward/iterative substitution)** junto con tácticas de cambio de variable.

Antes de sumergirnos en los métodos, recordemos la importancia de los **casos base (base cases)**. Toda recurrencia debe estar acompañada de condiciones para entradas pequeñas donde la recursión se detiene. Por ejemplo,

$T(1) = \Theta(1)$

suele ser el caso base indicando que para un tamaño mínimo el trabajo es constante. Los casos base garantizan que la definición recursiva no sea infinita y permiten iniciar cualquier demostración inductiva. En análisis asintótico, a menudo ignoramos las constantes de casos base por su efecto negligible en órdenes de crecimiento, pero al resolver formalmente una recurrencia (para obtener la fórmula exacta) sí se utilizan para determinar constantes particulares de la solución.

---

## Métodos para resolver recurrencias

A continuación, exploramos métodos comunes para resolver recurrencias y deducir la complejidad:

### Método de sustitución (Substitution method)

El método de sustitución consiste en **adivinar** una forma de solución para la recurrencia e intentar **probarla por inducción** matemática. Es decir, proponemos que $T(n)$ está acotada (por encima o debajo, dependiendo si buscamos cota superior Big-O o cota inferior) por alguna función candidata $f(n)$, y luego usamos inducción para ajustar constantes y validar la conjetura.

**Pasos del método de sustitución:**

1. **Hipótesis inductiva:** Suponer que para tamaños menores, la recurrencia cumple
   $T(k) \le c\,g(k)$
   (si buscamos cota superior) para cierta constante $c$ y toda $k < n$. Aquí $g(n)$ es nuestra conjetura de solución, típicamente una función simple como $n$, $n \log n$, $n^2$, etc.
    
2. **Paso inductivo:** Usar la ecuación de recurrencia de $T(n)$ y la hipótesis inductiva (que sustituye recursivamente $T$ de términos más pequeños) para demostrar que
   $T(n) \le c\,g(n)$
   también se cumple. Durante este paso se suelen necesitar ajustar los valores de $c$ (y a veces determinar a partir de qué tamaño $n \ge n_0$ se cumple la desigualdad).
    
3. **Verificar el caso base:** Comprobar explícitamente que la desigualdad es cierta para los casos base (y posiblemente algunos pequeños valores iniciales hasta $n_0$).

---

**Ejemplo con mergesort / divide y vencerás clásico**

Consideremos la recurrencia

$T(n) = 2\,T(n/2) + n$

con

$T(1) = 1$.

**Adivinamos** que

$T(n) = O(n \log n)$.

Es decir, conjeturamos

$T(n) \le c\,n \log_2 n$

para alguna constante $c > 0$. Para la **hipótesis inductiva**, asumimos que para todo $k < n$,

$T(k) \le c\,k \log_2 k$.

Entonces:

- Cuando $n > 1$, usando la recurrencia:

  $T(n) = 2\,T(n/2) + n.$

  Aplicando la hipótesis inductiva a $T(n/2)$ (suponiendo $n/2 \ge 1$):

  $T(n) \le 2 \big(c\,(n/2)\log_2(n/2)\big) + n = c\,n \log_2(n/2) + n.$

  Ahora simplificamos:
  $\log_2(n/2) = \log_2 n - \log_2 2 = \log_2 n - 1$.

  Por tanto,

  $T(n) \le c\,n (\log_2 n - 1) + n = c\,n \log_2 n - c\,n + n.$

  Para que esto sea
  $\le c\,n \log_2 n$,
  necesitamos que

  $-c\,n + n \le 0,$

  es decir,

  $n(1-c) \le 0.$

  Esto se cumple para $c \ge 1$. Si elegimos $c \ge 1$, obtenemos

  $T(n) \le c\,n \log_2 n$.

Solo falta verificar las bases: nuestro supuesto falla para $n = 1$ pues

$c\,1\log_2 1 = 0$

y

$T(1) = 1$.

Pero podemos ajustar tomando $n \ge 2$ como dominio de inducción y verificar manualmente

$T(2) \le c\,2\log_2 2 = 2c$.

Como

$T(2) = 2T(1) + 2 = 4$,

basta elegir $c$ suficientemente grande (por ejemplo $c = 4$) para que la desigualdad inicie en $n = 2$. Con esos ajustes, la inducción completa muestra

$T(n) = O(n \log n)$.

Además, como existe también una cota inferior $\Omega(n \log n)$ para esta recurrencia, en realidad

$T(n) = \Theta(n \log n)$.

---

**Advertencia – errores comunes:**

Al utilizar el método de sustitución es fácil probar una cota **demasiado floja** si la conjetura no es ajustada. Por ejemplo, uno podría “probar” sin dificultad que

$T(n) = O(n^2)$

para el mergesort anterior, lo cual es cierto pero no útil (es una cota muy débil comparada con $O(n \log n)$). Siempre busca la cota más ajustada (por lo general, $\Theta$).

Otro error típico es no manejar correctamente las constantes adicionales. En nuestro ejemplo, si hubiéramos obtenido

$T(n) \le c\,n\log_2 n - c\,n + n,$

no podemos simplemente descartar el término $-c\,n + n$ sin más, ya que “pequeños” excesos en cada nivel pueden acumularse. De hecho, suponer

$T(n) \le c n$

y reemplazar:

$T(n) = 2T(n/2) + n \le 2(c\,(n/2)) + n = c n + n.$

Esto muestra

$T(n) \le (c+1)n,$

que no es de la forma $c n$ requerida, fallando la inducción. La lección es que se debe probar exactamente la forma postulada; cualquier exceso constante debe ser absorbido eligiendo adecuadamente $c$ (por eso en inducciones fuertes suele usarse $\le c n - d$ con término corregido).

También es importante establecer claramente desde qué valor de $n$ vale la prueba inductiva (a menudo $n \ge n_0$), ya que las recurrencias se suelen analizar para $n$ grandes y uno puede permitirse excluir unos pocos casos pequeños.

En resumen, el método de sustitución es poderoso para demostrar cotas _si ya conocemos la solución_. Pero, ¿cómo adivinar $g(n)$? Ahí es donde ayuda el siguiente método.

---

### Método del árbol de recursión (Recursion tree)

Un **árbol de recursión** es una representación visual que desglosa la recurrencia en niveles de llamados recursivos. Cada nodo del árbol representa una llamada recursiva (subproblema) y está etiquetado con el **coste** de esa llamada (generalmente, el trabajo no recursivo realizado en esa llamada). Los hijos de un nodo representan las llamadas recursivas que éste hace. Sumando los costes de todos los nodos obtenemos el coste total del algoritmo. Este método permite **visualizar cómo se distribuye el trabajo** a lo largo de la recursión, facilitando conjeturar la solución.

**Construcción del árbol:** Dada $T(n)$ con recurrencia, dibujamos la raíz con costo $f(n)$ (coste de la parte no recursiva). Luego dibujamos sus hijos: cada hijo corresponde a una subllamada $T(\text{tamaño subproblema})$ con su costo. Continuamos expandiendo hasta alcanzar casos base. Por ejemplo, para

$T(n) = 2T(n/2) + n$:

- la raíz tiene costo $\Theta(n)$;
- genera 2 subproblemas de tamaño $n/2$, cada uno con costo $\Theta(n/2)$;
- estos a su vez generan subproblemas de tamaño $n/4$, etc.

_Figura mental:_ Árbol de recurrencia para mergesort $T(n) = 2T(n/2) + n$. Cada nodo muestra el tamaño del subproblema y el costo $\Theta(n)$ de combinar resultados en ese nivel. Cada nivel tiene costo total $\Theta(n)$ y hay $\log_2 n + 1$ niveles, resultando en costo total $\Theta(n \log n)$.

---

**Suma de costos por nivel:** En un árbol de recursión, es útil calcular el costo total en el nivel $i$. En el ejemplo de mergesort:

- Nivel 0 (raíz): 1 nodo de tamaño $n$, costo $\sim c n$ (para alguna constante $c$). Total nivel = $c n$.
- Nivel 1: 2 nodos, cada uno de tamaño $n/2$, costo $\sim c(n/2)$ cada uno. Total nivel = $2 \cdot c(n/2) = c n$.
- Nivel 2: 4 nodos, cada uno de tamaño $n/4$, costo $\sim c(n/4)$. Total nivel = $4 \cdot c(n/4) = c n$.

Observamos que **cada nivel** hasta las hojas suma aproximadamente $c n$. ¿Cuántos niveles hay? La profundidad es aquella $d$ donde los subproblemas se reducen a tamaño constante (caso base). Dado que el tamaño se divide entre 2 en cada nivel, se alcanza

$\dfrac{n}{2^d} = 1,$

de donde

$2^d = n$

y

$d = \log_2 n$.

Por tanto hay $\log_2 n$ niveles _recursivos_ más el nivel 0 inicial, en total $\approx \log_2 n + 1$ niveles. Sumando el costo de todos los niveles:

$(\log_2 n + 1)\cdot(c n) = c n \log_2 n + c n = \Theta(n \log n).$

Este resultado coincide con la solución que habíamos conjeturado.

En general:

- Si el **trabajo por nivel** resulta aproximadamente constante (como aquí, $\Theta(n)$ cada nivel), el costo total es niveles $\times$ costo por nivel = $\Theta(n \log n)$.
- Si el trabajo por nivel crece o decrece a medida que bajamos, hay que sumar una serie geométrica:
  - Si decrece geométricamente (cada nivel tiene mucho menos costo que el anterior), el costo total tiende a ser dominado por el nivel superior (caso “root-heavy” que veremos en el teorema maestro).
  - Si crece geométricamente (cada nivel más costoso que el anterior), el costo total lo dominan los niveles inferiores (caso “leaf-heavy”).
  - Si cada nivel cuesta lo mismo (caso equilibrado), el costo es niveles $\times$ (costo nivel), como en mergesort.

Usamos el árbol recursivo a menudo para _intuir_ la solución y luego confirmarla con sustitución. **Ventaja:** es muy visual y nos muestra cómo se distribuyen los costos. De hecho, el teorema maestro que veremos deriva de analizar el patrón de costos por nivel. **Precaución:** al dibujar el árbol podemos ignorar detalles como pisos o techos $\lfloor \cdot \rfloor$ en divisiones y constantes aditivas, ya que estos raramente afectan la complejidad asintótica final. Sin embargo, para una prueba formal, esas aproximaciones deben justificarse (generalmente, $n$ se toma potencia de 2 para simplificar, etc., y luego se puede argumentar que el resultado no cambia en casos generales).

---

**Ejemplo:** Consideremos la recurrencia

$T(n) = T(n/2) + T(n/2) + O(n)$

(parecida a quicksort promedio). Su árbol es similar al de mergesort: en cada nivel $i$ hay $2^i$ subproblemas de tamaño $n/2^i$, cada uno con costo $\Theta(n/2^i)$ en combinar (particionar o fusionar). El costo total por nivel es

$2^i \cdot \Theta(n/2^i) = \Theta(n),$

igual en todos los niveles, con $\sim \log_2 n$ niveles, total

$\Theta(n \log n)$.

En cambio, la recurrencia

$T(n) = T(n-1) + O(n)$

(peor caso de quicksort) produce un árbol muy desbalanceado: un camino lineal de profundidad $n$ donde el nivel 0 cuesta $\Theta(n)$, nivel 1 $\Theta(n-1)$, etc., sumando

$\Theta(n + (n-1) + \dots + 1) = \Theta(n^2).$

Así, el árbol “degenerado” revela la complejidad cuadrática.

---

### Teorema maestro (Master Theorem)

El teorema maestro es una **fórmula general** (a modo de _receta_) que da la solución asintótica para recurrencias divide-y-vencerás de la forma:

$T(n) = a\,T\!\left(\dfrac{n}{b}\right) + f(n),$

donde $a \ge 1$ es el número de subproblemas, $b > 1$ es el factor de reducción de tamaño de cada subproblema, y $f(n)$ es el costo de dividir y combinar los subproblemas (el trabajo no recursivo). Este teorema compara $f(n)$ con $n^{\log_b a}$, que corresponde (como vimos en el árbol) al tamaño total de trabajo en las hojas del árbol recursivo cuando cada hoja cuesta constante. En esencia, determina si la mayor carga de trabajo está en las **hojas** de la recursión, en la **raíz**, o está **equilibrada en todos los niveles**.

Sea

$c = \log_b a.$

Entonces se consideran tres casos principales:

---

#### Caso 1 (hojas dominantes, “leaf-heavy”)

Si $f(n)$ crece **más lento** que $n^c$ (polinómicamente más pequeño), específicamente si existe un $\epsilon > 0$ tal que

$f(n) = O\big(n^{c - \epsilon}\big)$

cuando $n$ tiende a infinito, entonces el costo lo dominan las hojas del árbol recursivo. Intuitivamente, cada nivel hace menos trabajo que el siguiente, por lo que la mayor parte del trabajo ocurre en el fondo. En este caso,

$T(n) = \Theta\big(n^c\big).$

Es decir, $T(n)$ asintóticamente está en el orden de $n^{\log_b a}$.

**Ejemplo:** búsqueda binaria tiene $a = 1,\; b = 2$, entonces

$c = \log_2 1 = 0.$

Su $f(n) = O(1)$ es $O(n^{0 + \epsilon})$ para cualquier $\epsilon > 0$ (de hecho $O(1) = O(n^\epsilon)$ para cualquier $\epsilon$ positivo). Cumple caso 1, y en efecto

$T(n) = \Theta(n^0) = \Theta(1)$

de trabajo por nivel, pero multiplicado por $\log n$ niveles da

$\Theta(\log n).$

(De forma más formal, el teorema maestro estándar tiene matices para $c = 0$, pero el resultado conocido es $T(n) = \Theta(\log n)$, como ya analizamos.)

---

#### Caso 2 (balanceado, trabajo uniforme por nivel)

Si $f(n)$ crece al mismo ritmo que $n^c$ (hasta factores logarítmicos), es decir, existe $k \ge 0$ tal que

$f(n) = \Theta\big(n^c \log^k n\big),$

entonces el trabajo en $f(n)$ es _comparable_ al total de las hojas. Equivale a que cada nivel del árbol aporta similar cantidad de trabajo. En este caso,

$T(n) = \Theta\big(n^c \log^{k+1} n\big).$

Aparece un factor $\log n$ extra.

**Ejemplo:** mergesort tiene $a = 2,\; b = 2 \Rightarrow c = \log_2 2 = 1$. Aquí

$f(n) = \Theta(n)$,

que es $\Theta(n^1 \log^0 n)$, con $k = 0$. Estamos en Caso 2, así que

$T(n) = \Theta\big(n^1 \log^{0+1} n\big) = \Theta(n \log n),$

consistente con el cálculo anterior.

Otro ejemplo: Supongamos

$T(n) = 2T(n/2) + n \log n.$

Aquí $n^c = n$. Vemos que $f(n) = n \log n$ es crecimiento logarítmico encima de $n$, es decir

$f(n) = \Theta(n \log^1 n)$

con $k = 1$. Aplica caso 2, dando

$T(n) = \Theta\big(n \log^{1+1} n\big) = \Theta\big(n (\log n)^2\big).$

---

#### Caso 3 (raíz dominante, “root-heavy”)

Si $f(n)$ crece **más rápido** que $n^c$ (polinómicamente mayor), es decir si existe $\epsilon > 0$ tal que

$f(n) = \Omega\big(n^{c + \epsilon}\big)$

_y además se cumple la condición de regularidad_, entonces el costo total está dominado por el trabajo **no recursivo** (las divisiones/combinaciones en niveles altos). En términos del árbol, el nivel raíz (o niveles superiores) hacen más trabajo que la suma de todos los niveles inferiores.

La condición de regularidad requiere que

$a\,f(n/b) \le K\,f(n)$

para alguna constante $K < 1$ y suficientemente grande $n$. Esto asegura que $f(n)$ no solo es mayor que $n^c$ en $n$ grande, sino que además la contribución de los niveles inferiores no “crece” de forma que sobrepase a $f(n)$.

Bajo estas condiciones,

$T(n) = \Theta\big(f(n)\big).$

**Ejemplo:** La recurrencia en el peor caso de quicksort:

$T(n) = T(n-1) + \Theta(n).$

Aquí no encaja directamente en la forma $T(n) = a T(n/b) + f(n)$ con $b>1$, pero si la pensamos, el término $f(n) = \Theta(n)$ domina en cada nivel, y cada nivel tiene costo lineal no decreciente. La suma es triangular:

$n + (n-1) + \dots + 1 = \Theta(n^2).$

En efecto, el resultado conocido es

$T(n) = \Theta(n^2).$

Otro ejemplo más limpio para el teorema maestro:

$T(n) = 3T(n/4) + n^2.$

Aquí $a = 3,\; b = 4$,

$c = \log_4 3 \approx 0.792.$

Y

$f(n) = n^2.$

Dado que $n^2$ crece mucho más rápido que $n^c \approx n^{0.792}$ (aquí $\epsilon \approx 1.208$), estamos en caso 3. Se verifica la condición de regularidad (la recursión reduce suficientemente el tamaño). Por tanto,

$T(n) = \Theta(n^2).$

En general, en caso 3 el término $f(n)$ domina tanto que la solución es esencialmente ese término.

---

**Resumen del teorema maestro en notación asintótica**

Para la recurrencia

$T(n) = aT\big(\dfrac{n}{b}\big) + f(n)$

con $a \ge 1,\; b > 1$ y $c = \log_b a$:

- Si
  $f(n) = O\big(n^{c - \epsilon}\big)$
  para algún $\epsilon > 0$, entonces

  $T(n) = \Theta\big(n^c\big).$
  _(Caso 1)_
    
- Si
  $f(n) = \Theta\big(n^c \log^k n\big)$
  para algún $k \ge 0$, entonces

  $T(n) = \Theta\big(n^c \log^{k+1} n\big).$
  _(Caso 2)_
    
- Si
  $f(n) = \Omega\big(n^{c + \epsilon}\big)$
  para algún $\epsilon > 0$ **y** se cumple la condición de regularidad
  $a\,f(n/b) \le K\,f(n)$
  para algún $K < 1$, entonces

  $T(n) = \Theta\big(f(n)\big).$
  _(Caso 3)_

> **Nota:** Este teorema tiene variantes y demostraciones clásicas (por inducción o derivando la suma del árbol recursivo). Si $f(n)$ no encaja bien en estos casos (por ejemplo, $f(n)$ está entre casos, como ligeramente mayor que $n^c$ sin factor polinomial claro, e.j.
> $f(n) = n^c \cdot \log \log n$),
> el teorema maestro básico no es directamente aplicable. En tales situaciones se pueden usar métodos manuales o el **método general de Akra–Bazzi**, que es una versión más poderosa que lidia con casos frontera y divisiones no exactas. Sin embargo, la mayoría de las recurrencias típicas de algoritmos sí encajan en el formato del teorema maestro o pueden ajustarse convenientemente.

---

### Sustitución iterativa y cambio de variable

La **sustitución iterativa (iterative substitution)**, a veces llamada _sustitución hacia atrás (backward substitution)_, es esencialmente “desenrollar” la recurrencia repetidamente para observar un patrón. Es muy similar a lo que hacemos conceptualmente con el árbol recursivo, pero de forma algebraica paso a paso. Consiste en sustituir la definición recursiva de $T(\cdot)$ en sí misma varias veces.

Veamos algunos ejemplos:

---

**Ejemplo 1:** $T(n) = T(n-1) + 1,$ con $T(1) = 1$.

Iteramos:

$T(n) = T(n-1) + 1.$

Sustituimos

$T(n-1) = T(n-2) + 1:$

$T(n) = [T(n-2) + 1] + 1 = T(n-2) + 2.$

Otra vez, usando

$T(n-2) = T(n-3) + 1:$

$T(n) = T(n-3) + 3.$

Continuamos hasta llegar a $T(1)$: después de $n-1$ sustituciones,

$T(n) = T(1) + (n-1)\cdot 1 = 1 + (n-1) = n.$

Por lo tanto

$T(n) = \Theta(n).$

Este resultado concuerda con la idea de que hacemos una operación por cada uno de los $n$ niveles de recursión.

---

**Ejemplo 2:** $T(n) = T(n/2) + 1,$ con $T(1) = 1$.

Aquí la variable decrece multiplicativamente. Iteramos:

$T(n) = T(n/2) + 1.$

Luego

$T(n/2) = T(n/4) + 1.$

Sustituyendo:

$T(n) = T(n/4) + 2.$

Otra iteración, usando

$T(n/4) = T(n/8) + 1:$

$T(n) = T(n/8) + 3.$

Tras $j$ iteraciones:

$T(n) = T(n/2^j) + j.$

Eventualmente

$\dfrac{n}{2^j} = 1$

cuando

$2^j = n,$

es decir

$j = \log_2 n.$

Entonces

$T(n) = T(1) + \log_2 n = 1 + \log_2 n = \Theta(\log n).$

Esto coincide con el razonamiento previo de que _binary search_ toma tiempo logarítmico.

---

**Ejemplo 3:** $T(n) = 2T(n/2) + n$ (otra vez mergesort)

Aplicamos sustitución iterativa:

$T(n) = 2T(n/2) + n.$

Sustituimos:

$T(n/2) = 2T(n/4) + n/2.$

Entonces:

$T(n) = 2[2T(n/4) + n/2] + n = 4T(n/4) + 2n + n.$

Repetimos este proceso $k$ veces:

$T(n) = 2^k T(n/2^k) + k n.$

Elegimos $k = \log_2 n$, de forma que

$n/2^k = 1$,

y obtenemos:

$T(n) = 2^{\log_2 n} T(1) + (\log_2 n) n = n T(1) + n \log_2 n.$

Como $T(1)$ es constante, esto es

$T(n) = n + n \log_2 n = \Theta(n \log n).$

La técnica iterativa puede formalizarse sumando la serie resultante. En el ejemplo anterior sumamos

$n + n + \dots$ ( $\log n$ veces ) $= n \log n$.

En general, suele dar lugar a series geométricas o aritméticas cuyo sumatorio conocemos.

---

**Cambio de variable**

En ocasiones, una recurrencia complicada se vuelve más manejable al redefinir la variable independiente. Esto es útil sobre todo cuando el argumento recursivo está en forma no polinómica, por ejemplo una raíz cuadrada, o una potencia.

Por ejemplo, consideremos la recurrencia:

$T(n) = T(\sqrt{n}) + 1.$

La “profundidad” de recursión es cuántas veces debemos tomar raíz cuadrada de $n$ hasta llegar al caso base. Si definimos

$m = \log_2 n$,

entonces

$n = 2^m$

y

$\sqrt{n} = \sqrt{2^m} = 2^{m/2}.$

En lugar de trabajar con $n$, definimos una nueva función $S(m) = T(2^m)$. Entonces la recurrencia en términos de $m$ se vuelve:

$S(m) = S(m/2) + 1.$

Esta recurrencia ya es del tipo “divide el tamaño entre 2” que sabemos resolver: por sustitución iterativa, obtenemos

$S(m) = \Theta(\log m).$

Volviendo a la variable original:

$m = \log_2 n,$

así que

$S(m) = T(2^m) = T(n) = \Theta(\log \log n).$

Es decir, el número de veces que puedes aplicar raíz cuadrada a $n$ hasta llegar a 2 es del orden de $\log \log n$. Esta técnica de cambio de variable es muy útil para lidiar con expresiones como $T(\sqrt{n})$, $T(\log n)$, etc., reduciendo la recurrencia a un formato ya conocido.


# Algoritmos Recursivos Clásicos y Análisis de Complejidad por Recurrencias
- mediante **inducción matemática (mathematical induction)**,
- o aplicando fórmulas conocidas como el **Teorema Maestro (Master Theorem)** cuando corresponde.

A continuación, presentamos varios algoritmos clásicos que utilizan recurrencias en su análisis de complejidad. Para cada algoritmo se proporciona:

- **Descripción del algoritmo** y su funcionamiento.  
- **Código de ejemplo en Java** con comentarios en español (y términos técnicos en inglés entre paréntesis).  
- **Recurrencia** que modela su tiempo de ejecución $T(n)$.  
- **Resolución de la recurrencia**, explicada paso a paso (expansión, inducción o Teorema Maestro según el caso).  
- **Análisis de complejidad temporal** en el mejor caso, peor caso y caso promedio (si aplica).  
- **Análisis de complejidad espacial (space complexity)**, considerando el uso de memoria adicional y la profundidad de la recursión.

Comenzaremos con casos sencillos (como factorial y Fibonacci) y avanzaremos a algoritmos de ordenamiento más complejos (Merge Sort, Quick Sort), finalizando con un problema clásico de recursión pura (Torres de Hanói). Al final, incluiremos también otros ejemplos recursivos opcionales (potenciación, búsqueda lineal, máximo de un arreglo).

---

## Factorial

### Descripción

El **factorial** de un número entero $n$ (denotado $n!$) se define como el producto de todos los enteros positivos desde $1$ hasta $n$. Por ejemplo:

- $5! = 5 \times 4 \times 3 \times 2 \times 1 = 120$.

Una forma natural de calcular el factorial es mediante un algoritmo recursivo:

- Caso base: $0! = 1$ (y usualmente también $1! = 1$).
- Paso recursivo: para $n > 1$, $n! = n \cdot (n-1)!$.

En forma recursiva, `factorial(n)` llama a `factorial(n-1)` y luego multiplica el resultado por $n$. Esto produce una secuencia lineal de llamadas recursivas decreciendo el valor de $n$ hasta alcanzar el caso base.

### Código en Java

```java
public class FactorialRecursivo {

    // Calcula el factorial de n de forma recursiva.
    // Por ejemplo, factorial(5) devuelve 120.
    public static long factorial(int n) {
        // Caso base: 0! = 1 y 1! = 1 (base case)
        if (n <= 1) {
            return 1;
        }
        // Llamada recursiva: n * factorial(n-1) (recursive call)
        return n * factorial(n - 1);
    }

    public static void main(String[] args) {
        int numero = 5;
        long resultado = factorial(numero);
        System.out.println("Factorial de " + numero + " = " + resultado);
    }
}
```


En el código anterior, la función `factorial(int n)` se llama a sí misma con `n-1` hasta alcanzar el caso base. Cada llamada recursiva realiza una multiplicación cuando vuelve de la recursión, construyendo el resultado final.

**Recurrencia:** Si denotamos $T(n)$ como el tiempo de ejecución para calcular `factorial(n)`, la relación de recurrencia que modela este algoritmo es:

$T(n) = T(n-1) + C$

para $n > 1$, con la condición inicial $T(1) = \Theta(1)$ (o $T(0) = \Theta(1)$) dado que los casos base retornan directamente. Aquí $C$ es el tiempo constante de la multiplicación y la gestión de la llamada (que consideramos $O(1)$). En esencia, por cada incremento en $n$ el algoritmo realiza una llamada recursiva adicional y una multiplicación constante.

**Resolución de la recurrencia:** Esta recurrencia se resuelve mediante **expansión iterativa**. Sustituyendo repetidamente $T(n-1)$ en la ecuación:

- $T(n) = T(n-1) + C$
    
- $T(n) = \big(T(n-2) + C\big) + C = T(n-2) + 2C$
    
- $T(n) = T(n-3) + 3C$
    
- ...
    
- $T(n) = T(n-k) + kC$
    

Eventualmente, cuando $k = n-1$ llegamos al caso base $T(1)$ (o $T(0)$):

$T(n) = T(1) + (n-1)C.$

$T(1)$ es una constante, así que ignorándola y tomando $C$ como constante, obtenemos:

$T(n) = (n-1)C + \text{constante} = \Theta(n).$

Por inducción también se puede demostrar que $T(n)$ crece de forma lineal con $n$. Intuitivamente, la función realiza $n-1$ multiplicaciones (más la verificación del caso base en cada llamada), por lo que el tiempo de ejecución es proporcional a $n$.

**Complejidad temporal:**

- **Mejor caso:** $O(1)$, ocurre cuando $n = 0$ o $n = 1$, ya que la función retorna de inmediato sin hacer llamadas recursivas adicionales.
    
- **Peor caso:** $O(n)$, ocurre para $n$ grande, donde la recursión se ejecuta $n$ veces (de $n$ hasta $1$) realizando $n-1$ multiplicaciones.
    
- **Caso promedio:** $O(n)$. Dado que para cualquier $n > 1$ la cantidad de trabajo crece linealmente con $n$ (no hay diferencias significativas entre distintas entradas del mismo tamaño), podemos decir que en promedio también es lineal.
    

En notación **Theta**, podemos afirmar más precisamente que el tiempo de ejecución del factorial recursivo es **$\Theta(n)$**, ya que crece linealmente con el tamaño de entrada $n$.

**Complejidad espacial:** La complejidad de espacio está dominada por la **pila de llamadas (call stack)** de la recursión. Cada llamada recursiva ocupa espacio en la memoria para los parámetros y variables locales. Para `factorial(n)`, la profundidad de la recursión es $n$ (en el peor caso, cuando $n$ decrece hasta $1$). Por tanto, el uso de espacio adicional es $O(n)$ debido a la pila de recursión. Aparte de eso, el algoritmo sólo utiliza unas pocas variables primitivas, lo que es espacio constante. No se utilizan estructuras de datos auxiliares que dependan de $n$. En resumen, la complejidad espacial es **$O(n)$** (lineal en $n$) debido a las $n$ llamadas anidadas en la stack (pila) de recursión.

## Fibonacci (sin _memoización_)

**Descripción del algoritmo:** La **sucesión de Fibonacci** es otra definición recursiva clásica. Los números de Fibonacci se definen como: $F(0) = 0$, $F(1) = 1$ y para $n \ge 2$, $F(n) = F(n-1) + F(n-2)$. La secuencia resultante comienza $0,1,1,2,3,5,8,13,\dots$ etc. Un algoritmo recursivo ingenuo para calcular $F(n)$ sigue directamente esta definición matemática: para obtener `fibonacci(n)` se invoca recursivamente `fibonacci(n-1)` y `fibonacci(n-2)` y se suman los resultados. Los casos base son $n = 0$ y $n = 1$, donde el resultado se conoce inmediatamente.

Este enfoque recursivo simple tiene la característica de recalcular muchas veces los mismos valores (por ejemplo `fibonacci(2)` se calculará múltiples veces como parte de `fibonacci(5)`, etc.). Veremos que esto provoca un crecimiento exponencial en el número de llamadas recursivas. A pesar de su ineficiencia, este algoritmo es útil para ilustrar análisis de recurrencias no lineales.

**Código en Java (recursivo sin memoización):**

```Java
public class FibonacciRecursivo {

    // Calcula el n-ésimo número de Fibonacci de forma recursiva (sin memoización).
    public static long fibonacci(int n) {
        // Caso base: fib(0) = 0, fib(1) = 1
        if (n < 2) {
            return n; // retorna n (0 o 1)
        }
        // Llamadas recursivas: fib(n-1) + fib(n-2)
        return fibonacci(n - 1) + fibonacci(n - 2);
    }

    public static void main(String[] args) {
        int n = 5;
        long resultado = fibonacci(n);
        System.out.println("Fibonacci(" + n + ") = " + resultado);
    }
}
```


En este código, `fibonacci(n)` llama recursivamente a `fibonacci(n-1)` y `fibonacci(n-2)` para computar el resultado. Por ejemplo, `fibonacci(5)` generaría llamadas a `fibonacci(4)` y `fibonacci(3)`; a su vez `fibonacci(4)` llama a `fibonacci(3)` y `fibonacci(2)`, etc. Esta explosión de llamadas resulta en un árbol binario de recursión.

**Recurrencia:** Si $T(n)$ es el tiempo de ejecución para calcular `fibonacci(n)` por este método recursivo, podemos describir su costo con la siguiente relación de recurrencia no lineal:

$T(n) = T(n-1) + T(n-2) + D,$

para $n \ge 2$, con condiciones iniciales $T(0) = \Theta(1)$ y $T(1) = \Theta(1)$ (los casos base realizan una comparación y retornan un valor). Aquí $D$ es el tiempo constante de hacer la suma y las operaciones básicas adicionales en cada nivel. Esta recurrencia refleja que para calcular `fibonacci(n)` hacemos dos llamadas recursivas independientes (una para $n-1$ y otra para $n-2$), además de una suma al combinar los resultados.

En forma simplificada, ignorando términos constantes, la recurrencia dominante es:

$T(n) \approx T(n-1) + T(n-2).$

Esta es esencialmente la **misma ecuación de recurrencia que la sucesión de Fibonacci** para el conteo de operaciones, lo cual sugiere un crecimiento exponencial en $n$.

**Resolución de la recurrencia:** Resolver exactamente $T(n) = T(n-1) + T(n-2) + O(1)$ produce una solución de orden **exponencial**. Podemos razonar de la siguiente manera: los valores de $T(n)$ crecerán proporcionalmente al número de llamadas recursivas realizadas. Si ignoramos constantes, la recurrencia homogénea asociada $T(n) = T(n-1) + T(n-2)$ tiene como solución general una combinación lineal de términos de la forma $r^n$, donde $r$ satisface la ecuación característica $r^2 = r + 1$. Las raíces de esta ecuación son

$r = \frac{1 \pm \sqrt{5}}{2}.$

La raíz dominante es $r \approx 1.618$ (conocido como el **número áureo** $\varphi$). Por tanto, asintóticamente $T(n)$ crecerá como $\varphi^n$ (en proporción a esta base exponencial). Más formalmente, la solución de la recurrencia es $T(n) = A \cdot \varphi^n + B \cdot \psi^n$ (donde $\psi \approx -0.618$ es la otra raíz), y para $n$ grandes el término $A \cdot \varphi^n$ domina. Así obtenemos $T(n) = \Theta(\varphi^n)$, que es **$\Theta(1.618^n)$**, es decir, **exponencial**. En notación Big-O a veces se simplifica diciendo que es $O(2^n)$, dado que la base $1.618^n$ está acotada por $2^n$ asintóticamente. En cualquier caso, el crecimiento es exponencial.

Otra forma de verlo es contar cuántas llamadas se realizan. Podemos demostrar por inducción que el número de sumas (u operaciones básicas) $S(n)$ necesarias para computar $F(n)$ por este método satisface $S(n) = S(n-1) + S(n-2) + 1$ (con $S(0) = 0$, $S(1) = 0$). Esta recurrencia $S(n)$ tiene la misma forma que Fibonacci pero desplazada, y de hecho $S(n)$ crece incluso más rápido que la propia secuencia de Fibonacci. Se puede probar que $S(n) > F(n)$ para $n > 2$, y dado que $F(n)$ es aproximadamente $\varphi^n / \sqrt{5}$, eso implica también un crecimiento exponencial para $S(n)$. En conclusión, el algoritmo recursivo simple de Fibonacci tiene **complejidad exponencial** en $n$.

**Complejidad temporal:**

- **Mejor caso:** $O(1)$. Ocurre en entradas triviales, cuando $n = 0$ o $n = 1$. En esos casos no se realizan llamadas recursivas adicionales, la función simplemente retorna $n$.
    
- **Peor caso:** $O(2^n)$ (más precisamente $\Theta(\varphi^n)$). Para valores grandes de $n$, el número de llamadas crece exponencialmente. Cada incremento en $n$ aproximadamente _duplica_ o más que duplica el trabajo, dado que la recurrencia tiene dos llamadas recursivas grandes. Esto resulta en un tiempo de ejecución exponencial.
    
- **Caso promedio:** En este algoritmo determinístico, el tiempo de ejecución depende directamente de $n$ (no de distribución aleatoria de datos), por lo que no hay variación entre casos "promedio" y peor: para un tamaño $n$ dado, siempre hará esencialmente el mismo número de llamadas (excepto por los casos base triviales). Podemos considerar que en promedio también requiere un tiempo exponencial para $n$ suficientemente grande.
    

En resumen, el método recursivo básico de Fibonacci es **muy ineficiente**, con una complejidad temporal **exponencial**. Esto empeora rápidamente a medida que $n$ crece.

**Complejidad espacial:** La complejidad espacial de este algoritmo también es alta debido a la recursión profunda. En el peor caso, la profundidad de la recursión es $n$ (por ejemplo, para calcular `fibonacci(n)` se anidan llamadas hasta llegar a `fibonacci(1)` y `fibonacci(0)` simultáneamente). En términos de espacio adicional, cada llamada recursiva ocupa espacio en la pila. La **profundidad máxima de la pila** aquí es $n$ (porque aunque se bifurcan las llamadas, el _branching_ se maneja secuencialmente: el camino más largo desde la raíz del árbol de recursión hasta una hoja tiene longitud $n$). Por lo tanto, el espacio auxiliar por las llamadas es $O(n)$. Aparte de eso, no se utilizan estructuras adicionales que crezcan con $n$ (solo variables escalares para parámetros y retornos). Entonces, la complejidad espacial es **$O(n)$** debido a la pila de recursión. (Nota: En general, para la mayoría de funciones recursivas puras, la memoria consumida es proporcional a la profundidad de recursión, aquí lineal en $n$).

## Fibonacci (con _memoización_)

**Descripción del algoritmo:** El algoritmo anterior de Fibonacci repite muchos cálculos redundantes. **Memoización** es una técnica de **optimización mediante almacenamiento en caché (caching)** que permite guardar resultados intermedios de llamadas recursivas para evitar recomputarlos. En el contexto de Fibonacci, podemos guardar los valores ya calculados de $F(k)$ en una estructura (por ejemplo un array o mapa) de modo que cada número de Fibonacci se calcule **solo una vez**. Esto convierte el algoritmo de exponencial a lineal, ya que las llamadas se reducen drásticamente.

La idea es: si ya calculamos $F(n-1)$ o $F(n-2)$ previamente, no hacemos la llamada recursiva de nuevo, sino que simplemente reutilizamos el resultado almacenado. Podemos implementar esto fácilmente usando un arreglo estático de tamaño $n+1$ que inicialmente marque valores no calculados, o utilizando recursividad con un arreglo auxiliar. También se puede transformar en un algoritmo **iterativo de programación dinámica (dynamic programming)**, pero aquí mantendremos la recursividad para ilustrar la mejora.

**Código en Java (recursivo con memoización):**

```Java
import java.util.Arrays;

public class FibonacciMemo {

    // Array para almacenar resultados calculados (memoización)
    private static long[] memo;

    public static long fibonacciMemo(int n) {
        // Inicializar la tabla de memo si es la primera llamada
        if (memo == null) {
            memo = new long[n + 1];
            Arrays.fill(memo, -1); // -1 indica no calculado
        }
        // Caso base: fib(0) = 0, fib(1) = 1
        if (n < 2) {
            return n;
        }
        // Si ya está calculado, devolverlo directamente (cached result)
        if (memo[n] != -1) {
            return memo[n];
        }
        // Calcular recursivamente y almacenar el resultado en memo[n]
        memo[n] = fibonacciMemo(n - 1) + fibonacciMemo(n - 2);
        return memo[n];
    }

    public static void main(String[] args) {
        int n = 50;
        long resultado = fibonacciMemo(n);
        System.out.println("Fibonacci(" + n + ") = " + resultado);
    }
}
```



En este código, usamos un arreglo `memo` para guardar los resultados de `fibonacciMemo(n)` a medida que se calculan. La primera vez que se llama a `fibonacciMemo(n)` se inicializa el arreglo con un tamaño suficiente ($n+1$) y valores por defecto (`-1` para indicar "no calculado"). Luego, cada vez que se calcula un valor, se almacena en `memo[n]`. Si una función recursiva va a calcular `fibonacciMemo(k)` y detecta que `memo[k]$ ya tiene un valor, simplemente lo retorna sin seguir recursando. Esto elimina las recalculaciones redundantes.

**Recurrencia:** Con la memoización, la **estructura del cálculo cambia**. En el peor de los casos, la recursión seguirá explorando todas las ramas necesarias para calcular los Fibonacci de $0$ hasta $n$ la primera vez. Sin embargo, **cada valor** $F(k)$ para $0 \le k \le n$ se calcula **una única vez**. Después de eso, las llamadas posteriores son resueltas en tiempo constante consultando el array. Para analizar el tiempo de ejecución, podemos pensar que esencialmente el algoritmo realiza un trabajo lineal: computa $F(0), F(1), ..., F(n)$ cada uno una vez (salvo los casos base trivialmente).

Formalmente, podríamos intentar describir la recurrencia contando solo las _nuevas_ computaciones. Podríamos decir que:

- $T(0) = \Theta(1)$, $T(1) = \Theta(1)$.
    
- Para $n > 1$, si el valor no está en `memo` (primer cálculo), entonces $T(n) = T(n-1) + T(n-2) + O(1)$. Pero una vez calculado, consultas posteriores a $T(n)$ son $O(1)$.
    

Esto es un poco complejo de modelar directamente con una sola recurrencia, pero otra forma de verlo es: el algoritmo con memoización realiza dos llamadas recursivas por cada número de Fibonacci **hasta que llegue a los casos base**. Pero gracias al cache, cada llamada para un subproblema dado ocurre solo la primera vez; las posteriores retornan en $O(1)$. El patrón de llamadas "primera vez" aún forma un árbol binario, pero _cada nodo de ese árbol se computa solo una vez_. El número total de nodos distintos en ese árbol es $n+1$ (valores de $0$ a $n$). Por lo tanto, la complejidad es aproximadamente proporcional a $n$.

En resumen, **el tiempo de ejecución con memoización es lineal**, $T(n) = O(n)$, dado que esencialmente estamos calculando secuencialmente todos los Fibonacci menores o iguales a $n$. Esta es también la complejidad de la solución iterativa óptima. La memoización transforma un problema exponencial en uno polinomial (en este caso lineal) al evitar recomputos.

**Resolución mediante inspección:** Podemos argumentar que el algoritmo recorre $n$ niveles "únicos". Pensemos que $C(n)$ es el conjunto de llamadas únicas que se hacen para calcular $F(n)$ con memo. Claramente $C(n) = {F(n), F(n-1), ..., F(1), F(0)}$, porque para calcular $F(n)$ necesitaremos primero $F(n-1)$ y $F(n-2)$ (primeros cálculos), luego para calcular cada uno de esos se necesitarán sus predecesores y así sucesivamente, abarcando toda la serie desde $0$ hasta $n$. Una vez que $F(k)$ está en `memo`, cualquier llamada posterior a ese valor termina instantáneamente. Por ello, el trabajo total es proporcional a $n$ (más específicamente, es $2n-1$ sumas si contamos sumas, lo cual es $O(n)$). También se puede formular un **análisis por inducción**: asumiendo que calcular todos los $F$ hasta $n-1$ cuesta $O(n-1)$, entonces calcular hasta $n$ requiere una suma adicional y ya los anteriores están listos, por lo que es $O(n)$ en total.

**Complejidad temporal:**

- **Mejor caso:** $O(1)$. Similar al caso anterior, si $n$ es $0$ o $1$, la función retorna inmediatamente (incluso con memoización no hay sobrecosto significativo en esos casos base).
    
- **Peor caso:** $O(n)$. Para $n$ grande, el algoritmo realiza cálculos para cada valor intermedio una sola vez. La complejidad es lineal en $n$.
    
- **Caso promedio:** $O(n)$. Dado que el comportamiento no varía significativamente para distintos patrones de entrada (solo depende de $n$), consideramos que en promedio también el tiempo crece linealmente con $n$.
    

En síntesis, el algoritmo recursivo de Fibonacci con memoización tiene **complejidad temporal lineal** $\Theta(n)$, lo cual es una mejora enorme sobre el $\Theta(\varphi^n)$ exponencial del método sin memo. Básicamente hemos aplicado **programación dinámica (dynamic programming)** almacenando resultados parciales.

**Complejidad espacial:** En este enfoque con memoización, usamos espacio adicional para almacenar los resultados calculados. Específicamente, utilizamos un arreglo `memo` de tamaño $n+1$ (índices $0$ a $n$) para guardar cada $F(i)$. Esto ocupa **$O(n)$** espacio adicional. Además, la recursión en sí misma sigue teniendo una profundidad lineal en $n$ (en el primer recorrido de cálculos); sin embargo, una vez que los valores están memorizados, las llamadas subsiguientes retornan rápidamente sin profundizar. En el peor caso inicial, la profundidad de la pila de recursión será $n$ (igual que antes), implicando $O(n)$ espacio por la pila. Luego, cada llamada "cacheada" es constante, pero esas no incrementan la profundidad. En total, la complejidad espacial es **$O(n)$** dominada por el almacenamiento de la tabla de memoización y/o la profundidad de recursión.

Si en lugar de recursividad hubiésemos implementado un algoritmo iterativo de Fibonacci (sumando en un bucle), la complejidad espacial sería $O(1)$ porque solo se necesitan dos variables para mantener los valores previos. Sin embargo, con la versión recursiva + memo presentada, sacrificamos un poco de espacio para preservar la recursividad: $O(n)$ espacio es requerido en total.

## Búsqueda Binaria (_Binary Search_)

**Descripción del algoritmo:** La **búsqueda binaria** es un algoritmo eficiente para buscar un elemento en un arreglo ordenado. Funciona con la técnica de **divide y vencerás (divide and conquer)**, eliminando la mitad de los elementos en cada paso. El proceso es: dado un arreglo ordenado y un valor objetivo, se compara el valor buscado con el elemento del medio del arreglo. Si son iguales, se encontró la posición; si el valor buscado es menor que el medio, entonces cualquier elemento mayor que el medio (incluida la mitad derecha del arreglo) queda descartado; si es mayor, se descarta la mitad izquierda. Luego, el algoritmo repite la búsqueda en la mitad restante. Este procedimiento recursivo continúa hasta encontrar el elemento o hasta que la sublista de búsqueda se vacía.

La búsqueda binaria es notable por su eficiencia: en un arreglo de tamaño $n$, típicamente encuentra el elemento (o determina su ausencia) en alrededor de $\log_2 n$ pasos. Implementaremos la versión recursiva para ilustrar la recurrencia, aunque también puede implementarse de forma iterativa fácilmente.

**Código en Java (búsqueda binaria recursiva):**

```Java
public class BusquedaBinaria {

    // Realiza búsqueda binaria recursiva en un arreglo ordenado.
    // Devuelve el índice del elemento buscado, o -1 si no se encuentra.
    public static int buscar(int[] arr, int objetivo, int izquierda, int derecha) {
        if (izquierda > derecha) {
            return -1; // caso base: no encontrado (empty subarray)
        }
        int medio = (izquierda + derecha) / 2;
        // Caso base: elemento encontrado en la posición 'medio'
        if (arr[medio] == objetivo) {
            return medio;
        }
        // Si el objetivo es menor, buscar en la mitad izquierda
        else if (objetivo < arr[medio]) {
            return buscar(arr, objetivo, izquierda, medio - 1);
        }
        // Si el objetivo es mayor, buscar en la mitad derecha
        else {
            return buscar(arr, objetivo, medio + 1, derecha);
        }
    }

    public static void main(String[] args) {
        int[] datos = {2, 5, 8, 12, 16, 23, 38, 56, 72, 91};
        int objetivo = 23;
        int indice = buscar(datos, objetivo, 0, datos.length - 1);
        if (indice != -1) {
            System.out.println("Elemento encontrado en el índice " + indice);
        } else {
            System.out.println("Elemento no encontrado");
        }
    }
}
```


En este código, la función `buscar` toma los límites `izquierda` y `derecha` de la porción del arreglo donde se busca. Calcula un índice `medio` y compara `arr[medio]` con el valor `objetivo`. Si lo encuentra, retorna el índice. Si el objetivo es menor, recurre a la sublista de la izquierda (`izquierda` a `medio-1`); si es mayor, recurre a la sublista de la derecha (`medio+1` a `derecha`). El caso base ocurre cuando la sublista vacía (`izquierda > derecha`) indica que el elemento no está presente. Debido a que en cada llamada recursiva el espacio de búsqueda se reduce a la mitad, esperamos una complejidad logarítmica.

**Recurrencia:** Si $T(n)$ es el tiempo de ejecutar búsqueda binaria en un arreglo de tamaño $n$, la recurrencia viene dada por:

$T(n) = T(n/2) + C,$

donde $C$ es un costo constante (comparación y cálculo del índice medio). La razón es que en cada paso descarta la mitad del array y continúa buscando en la otra mitad. Además, $T(1) = \Theta(1)$ (en un array de tamaño $1$ solo hará una comparación). Formalmente, podríamos escribir $T(n) = T(\lfloor n/2 \rfloor) + O(1)$.

**Resolución de la recurrencia:** Esta recurrencia se puede resolver de varias formas:

- **Por expansión:** Expandamos la recurrencia varias veces.  
    $T(n) = T(n/2) + C$  
    $T(n) = \big(T(n/4) + C\big) + C = T(n/4) + 2C$  
    $T(n) = T(n/8) + 3C$  
    ... tras $k$ expansiones, $T(n) = T(n/2^k) + kC$.
    
    Eventualmente, después de $k$ pasos la subproblema tiene tamaño $n/2^k = 1$ (o caso base). Igualando: $n/2^k = 1 \Rightarrow 2^k = n \Rightarrow k = \log_2 n$. Entonces  
    $T(n) = T(1) + (\log_2 n) C = \Theta(1) + C \log_2 n = \Theta(\log n)$.
    
- **Por el Teorema Maestro:** Aquí $a = 1$ (una sola subllamada), $b = 2$ (tamaño reducido a la mitad), y $f(n) = \Theta(1)$. Calculamos $n^{\log_b a} = n^{\log_2 1} = n^0 = 1$. Comparamos $f(n)$ con $n^0$: $f(n)$ es $\Theta(n^0)$ con $c = 0$ (es exactamente del mismo orden que $n^0$). Esto corresponde al caso 2 del Teorema Maestro (cuando $f(n) = \Theta(n^{\log_b a} \log^k n)$ con $k = 0$). Por tanto, $T(n) = \Theta(n^{\log_2 1} \log^{0+1} n) = \Theta(\log n)$.
    

De cualquier forma, concluimos que $T(n) = \Theta(\log n)$. Es decir, la búsqueda binaria tiene **complejidad logarítmica** en tiempo.

**Complejidad temporal:**

- **Mejor caso:** $O(1)$. Sucede si por casualidad el elemento buscado está justo en la posición media del arreglo en la primera iteración de búsqueda. En ese caso, una sola comparación lo encuentra y la búsqueda termina inmediatamente.
    
- **Peor caso:** $O(\log n)$. Esto ocurre cuando el elemento no está en el arreglo, o está en una posición que hace que la búsqueda tenga que profundizar al máximo. En cada paso se descarta la mitad, así que el número máximo de comparaciones es $\lceil \log_2 n \rceil$.
    
- **Caso promedio:** $O(\log n)$. En promedio, asumiendo que la posición del elemento objetivo es igualmente probable en cualquier lugar del arreglo (o que no esté), la búsqueda binaria aún realizará un número de pasos del orden de $\log n$.
    

En resumen, la búsqueda binaria es **muy eficiente**, con tiempo de ejecución **$\Theta(\log n)$** en el peor y promedio, significativamente mejor que la búsqueda lineal $O(n)$ en arreglos no ordenados.

**Complejidad espacial:** En su implementación recursiva, la búsqueda binaria utiliza la pila de recursión. La profundidad de la recursión en el peor caso es $\lceil \log_2 n \rceil$ (la misma cantidad de pasos que de comparaciones). Por lo tanto, el espacio adicional por la recursión es $O(\log n)$ (cada llamada ocupa un frame en la pila). Sin embargo, aparte de la recursividad, el algoritmo no usa estructuras auxiliares de tamaño proporcional a $n$; solo unas pocas variables (índices y límites). Si consideramos la implementación _iterativa_, la búsqueda binaria requiere **$O(1)$** espacio extra en total. Incluso en la forma recursiva, muchos análisis citan la **complejidad espacial** como $O(1)$ porque el almacenamiento adicional aparte de la entrada es constante (la recursión se puede considerar un costo de memoria logarítmico muy pequeño comparado con $n$). En sentido estricto:

- Versión recursiva: espacio $O(\log n)$ por la profundidad de llamada.
    
- Versión iterativa: espacio $O(1)$ (constante).
    

De cualquier modo, no se requiere espacio proporcional a $n$. La búsqueda binaria es in-situ y **espacialmente eficiente**.

## Merge Sort (Ordenamiento por mezcla)

**Descripción del algoritmo:** **Merge Sort** (ordenamiento por mezcla) es un algoritmo de ordenamiento que sigue el paradigma **divide y vencerás (divide and conquer)**. Divide repetidamente la lista en mitades hasta llegar a sublistas de tamaño $1$, que se consideran ordenadas, y luego _mezcla_ (merge) estas sublistas para producir sublistas mayores ordenadas, reconstruyendo el arreglo ordenado completo. Fue desarrollado por John von Neumann en 1945 y es conocido por su eficiencia consistente de $O(n \log n)$ en todos los casos.

El algoritmo funciona así:

1. Si la lista tiene $0$ o $1$ elementos, ya está ordenada (caso base).
    
2. Si tiene más elementos, dividir la lista en dos mitades aproximadamente iguales.
    
3. Ordenar recursivamente la primera mitad con Merge Sort.
    
4. Ordenar recursivamente la segunda mitad con Merge Sort.
    
5. **Fusionar (merge)** las dos mitades ordenadas en una sola lista ordenada.
    

La fase clave es la mezcla de dos listas ordenadas: se toman los elementos menores sucesivamente de la cabecera de ambas sublistas y se van colocando en la lista resultante. Esta fusión tiene un costo lineal en la suma de los tamaños de las sublistas.

**Código en Java (Merge Sort recursivo):**

```Java
public class MergeSort {

    // Función principal de Merge Sort: ordena arr[l..r] (inclusive)
    public static void mergeSort(int[] arr, int l, int r) {
        if (l < r) {
            int m = (l + r) / 2;
            // Ordenar la mitad izquierda (left half) recursivamente
            mergeSort(arr, l, m);
            // Ordenar la mitad derecha (right half) recursivamente
            mergeSort(arr, m + 1, r);
            // Combinar (merge) las dos mitades ordenadas
            merge(arr, l, m, r);
        }
    }

    // Función auxiliar para mezclar dos subarrays ordenados:
    // arr[l..m] y arr[m+1..r], resultado se coloca en arr[l..r] ordenado.
    private static void merge(int[] arr, int l, int m, int r) {
        int n1 = m - l + 1; // tamaño de sublista izquierda
        int n2 = r - m;     // tamaño de sublista derecha

        // Arrays temporales para copiar elementos
        int[] izquierda = new int[n1];
        int[] derecha = new int[n2];

        // Copiar datos a arrays temporales
        for (int i = 0; i < n1; i++) {
            izquierda[i] = arr[l + i];
        }
        for (int j = 0; j < n2; j++) {
            derecha[j] = arr[m + 1 + j];
        }

        // Fusionar los arrays temporales de vuelta en arr[l..r]
        int i = 0, j = 0;
        int k = l;

        while (i < n1 && j < n2) {
            if (izquierda[i] <= derecha[j]) {
                arr[k++] = izquierda[i++];
            } else {
                arr[k++] = derecha[j++];
            }
        }

        // Copiar los elementos restantes, si quedaron
        while (i < n1) {
            arr[k++] = izquierda[i++];
        }
        while (j < n2) {
            arr[k++] = derecha[j++];
        }
    }

    public static void main(String[] args) {
        int[] datos = {38, 27, 43, 3, 9, 82, 10};
        mergeSort(datos, 0, datos.length - 1);
        System.out.println("Array ordenado: " + java.util.Arrays.toString(datos));
    }
}
```


Comentarios sobre el código:

- `mergeSort(arr, l, r)` divide el array `arr` en dos partes: `l..m` y `m+1..r` (donde $m$ es el punto medio). Luego las ordena recursivamente y finalmente llama a `merge` para fusionarlas.
    
- La función `merge` combina dos subarreglos ordenados (`izquierda` y `derecha`) en el arreglo original `arr`. Usa índices `i` y `j` para recorrer los subarrays temporales, y `k` para la posición de inserción en `arr`. Compara los elementos frontales y copia el menor, avanzando los índices. Una vez que uno de los subarrays se vacía, copia los restantes del otro subarray.
    
- Esta implementación usa arreglos auxiliares para la fusión por simplicidad (lo cual ocupa espacio extra $O(n)$). Hay implementaciones _in-place_ más avanzadas, pero típicamente Merge Sort utiliza memoria auxiliar proporcional a $n$.
    

**Recurrencia:** Merge Sort divide la lista en dos sublistas de tamaño aproximadamente $n/2$, las ordena recursivamente y luego **mezcla** las dos listas ordenadas en tiempo $\Theta(n)$ (lineal en el tamaño de ambas). Entonces, si $T(n)$ es el tiempo de ordenar $n$ elementos con Merge Sort, se cumple:

$T(n) = T(n/2) + T(n/2) + \Theta(n) = 2T(n/2) + \Theta(n).$

Con $T(1) = \Theta(1)$ como caso base (una lista de un elemento está ordenada y no requiere computación). Esta es una recurrencia típica de algoritmos de ordenamiento por dividir en mitades.

**Resolución de la recurrencia:** Podemos resolver $T(n) = 2 T(n/2) + cn$ (donde $cn$ representa el costo lineal de la etapa de mezcla) usando el Teorema Maestro o por expansión:

- **Teorema Maestro:** Aquí $a = 2$, $b = 2$, por lo que $n^{\log_b a} = n^{\log_2 2} = n^1 = n$. Tenemos $f(n) = \Theta(n)$. Entonces $f(n)$ es $\Theta(n^{\log_b a} \cdot \log^k n)$ con $k = 0$ (ya que $f(n)$ es $\Theta(n^1)$ que coincide exactamente con $n^1$). Por el caso 2 del teorema, $T(n) = \Theta(n^1 \log n)$. Es decir, $T(n) = \Theta(n \log n)$.
    
- **Por expansión (suma de costos por nivel):** Otra forma instructiva es considerar el árbol de recursión: en el nivel $0$ (raíz) se hace $\Theta(n)$ trabajo para mezclar. En el nivel $1$, hay $2$ llamadas recursivas, cada una maneja $n/2$ elementos, cada una realizando $\Theta(n/2)$ en su mezcla; en total, el trabajo en nivel $1$ es $2 \cdot \Theta(n/2) = \Theta(n)$. En el nivel $2$, hay $4$ subproblemas de tamaño $n/4$, cada uno con costo $\Theta(n/4)$ para mezclar, sumando $4 \cdot \Theta(n/4) = \Theta(n)$. Vemos el patrón: **en cada nivel** el costo combinado de las fusiones es proporcional a $n$. ¿Cuántos niveles hay? La división continúa hasta subproblemas de tamaño $1$. Dado que dividimos $n$ sucesivamente por $2$, el número de niveles es $\log_2 n$. Entonces, sumando el costo de todos los niveles:
    

Costo total $= \Theta(n) + \Theta(n) + \dots + \Theta(n)$ (log $n$ veces) $= \Theta(n \log n).$

Efectivamente, el algoritmo ejecuta $\log_2 n$ niveles de recursión, cada nivel realizando un trabajo lineal $n$. Por eso, Merge Sort ejecuta en tiempo **$\Theta(n \log n)$**.

**Complejidad temporal:**

- **Mejor caso:** $O(n \log n)$. Merge Sort realiza prácticamente el mismo conjunto de operaciones independientemente del orden inicial de los datos (incluso si el array ya estuviera ordenado, igualmente lo dividiría y mezclaría). En teoría, el mejor caso también es $\Theta(n \log n)$.
    
- **Peor caso:** $O(n \log n)$. Merge Sort siempre divide y mezcla de la misma manera, por lo que no tiene un peor caso más costoso que el promedio.
    
- **Caso promedio:** $O(n \log n)$. Para entradas aleatorias típicas, Merge Sort ejecutará su proceso completo de dividir y mezclar, resultando siempre en tiempo casi lineal-logarítmico.
    

**Complejidad espacial:** Merge Sort típicamente requiere espacio auxiliar adicional para realizar la mezcla. En la implementación anterior, usamos arreglos temporales `izquierda[]` y `derecha[]` para ayudar en el merge. Esto implica que en cada nivel de recursión estamos usando memoria proporcional al tamaño de las sublistas que se están fusionando. En total, la cantidad máxima de memoria extra utilizada es del orden de $n$ (por ejemplo, en la fusión final se usan arreglos temporales de tamaño $\approx n/2$ cada uno, sumando $\approx n$). Además, la recursión tiene una profundidad $O(\log n)$, lo que añade un coste de pila de llamadas también $O(\log n)$. En general, Merge Sort se considera que requiere **$\Theta(n)$ espacio auxiliar** en arreglos.

En detalle:

- **Memoria auxiliar para merge:** $\Theta(n)$ en el peor caso, para combinar las mitades en cada nivel.
    
- **Pila de recursión:** profundidad $\log n$, con un overhead de variables locales constante por nivel, o sea $O(\log n)$ adicional, que es despreciable comparado con $n$ para valores grandes.
    

Por lo tanto, la complejidad espacial total de Merge Sort es **$O(n)$**.

## Quick Sort (Ordenamiento Rápido)

**Descripción del algoritmo:** **Quick Sort** es uno de los algoritmos de ordenamiento más populares y rápidos en promedio. También sigue divide y vencerás, pero a diferencia de Merge Sort, la mayor parte del trabajo se realiza _antes_ de las llamadas recursivas, durante la fase de **particionado (partition)**. Quick Sort trabaja así:

1. Si la lista tiene $0$ o $1$ elementos, ya está ordenada.
    
2. Si tiene $2$ o más elementos, seleccionar un **pivote (pivot)**. (Por simplicidad usualmente se elige el último elemento de la lista, aunque la elección del pivote influye en el rendimiento).
    
3. **Particionar** el arreglo en dos partes: todos los elementos menores que el pivote se colocan a la izquierda, y todos los mayores a la derecha. El pivote termina en su posición final ordenada. Esta operación se hace _in-place_ intercambiando elementos. Al final de la partición, el pivote divide la lista en dos sublistas (elementos a la izquierda del pivote y elementos a la derecha).
    
4. Aplicar Quick Sort recursivamente a la sublista izquierda (elementos menores) y a la sublista derecha (elementos mayores).
    
5. No es necesario combinar, porque la lista queda ordenada después de que ambas recursiones terminen (cada elemento ya está en su lugar definitivo relativo al pivote).
    

La eficiencia de Quick Sort depende de qué tan bien el pivote logra dividir la lista. Idealmente parte la lista a la mitad (como Merge Sort). En el peor caso, el pivote podría resultar ser el elemento más grande o más pequeño, dejando una partición muy desequilibrada (por ejemplo, sublista izquierda de tamaño $n-1$ y derecha de tamaño $0$), lo que degrada la eficiencia. Sin embargo, con estrategias de pivote aleatorio o "mediana de tres", Quick Sort logra un rendimiento promedio muy bueno.

A diferencia de Merge Sort, Quick Sort suele implementarse de modo que **no requiere espacio auxiliar adicional significativo**, ya que el particionado se hace sobre el mismo arreglo (in-place).

**Código en Java (Quick Sort con partición Lomuto):**

```Java
public class QuickSort {

    public static void quickSort(int[] arr, int low, int high) {
        if (low < high) {
            // Particionar el arreglo y obtener el índice del pivote colocado correctamente
            int pi = partition(arr, low, high);
            // Ordenar recursivamente las dos subpartes
            quickSort(arr, low, pi - 1);   // sublista izquierda
            quickSort(arr, pi + 1, high);  // sublista derecha
        }
    }

    // Función de partición (esquema de Lomuto)
    private static int partition(int[] arr, int low, int high) {
        int pivot = arr[high];   // Elegir pivote como último elemento
        int i = low;             // Índice para elementos menores que el pivote

        for (int j = low; j < high; j++) {
            if (arr[j] < pivot) {
                // intercambiar arr[i] y arr[j]
                int temp = arr[i];
                arr[i] = arr[j];
                arr[j] = temp;
                i++;
            }
        }

        // Colocar el pivote en la posición correcta (índice i)
        int temp = arr[i];
        arr[i] = arr[high];
        arr[high] = temp;
        return i;  // retornar la posición final del pivote
    }

    public static void main(String[] args) {
        int[] datos = {10, 7, 8, 9, 1, 5};
        quickSort(datos, 0, datos.length - 1);
        System.out.println("Array ordenado: " + java.util.Arrays.toString(datos));
    }
}
```



Explicación del código:

- `quickSort(arr, low, high)` es la función recursiva principal que ordena el segmento `arr[low..high]`. Si `low < high`, entonces hay más de un elemento y necesitamos particionar.
    
- `partition(arr, low, high)` elige un pivote (aquí el elemento `arr[high]`) y reorganiza el segmento para que todos los menores al pivote queden a su izquierda y los mayores a la derecha. Devuelve el índice final `pi` donde quedó colocado el pivote.
    
- Luego Quick Sort llama recursivamente a sí mismo para la porción izquierda (antes del pivote) y la porción derecha (después del pivote).
    
- Este esquema de partición es el **Lomuto partition scheme**. Otros esquemas (como Hoare) existen y tienen ligeras diferencias en eficiencia, pero logran lo mismo.
    
- Después de la partición, el pivote está en posición correcta definitiva. No es necesario mezclar como en Merge Sort; las recursiones procesan independientemente cada lado.
    

**Recurrencia:** El tiempo de Quick Sort es un poco más complejo de modelar porque **depende del punto de partición**. En el caso general, si al hacer `partition` el pivote termina en posición $k$ (0-indexed, relativo al segmento), tendremos un subproblema de tamaño $k$ y otro de tamaño $n-k-1$ (excluyendo el pivote). El costo de la partición es $\Theta(n)$ (recorre la lista comparando e intercambiando). Entonces podríamos escribir la recurrencia:

$T(n) = T(k) + T(n-k-1) + \Theta(n),$

para algún $0 \le k < n-1$.

En el **mejor caso**, el pivote divide el array casi en mitades iguales. Eso implicaría $k \approx \frac{n-1}{2}$ (o exactamente $\lfloor (n-1)/2 \rfloor$ y $\lceil (n-1)/2 \rceil$ elementos a cada lado). En ese caso, la recurrencia aproximada es:

$T(n) = 2T(n/2) + \Theta(n),$

que es la misma forma que Merge Sort, dando $T(n) = \Theta(n \log n)$.

En el **peor caso**, el pivote resulta extremadamente desbalanceado, digamos $k = n-1$ (o $k = 0$, simétricamente). Entonces la recurrencia se convierte en:

$T(n) = T(n-1) + T(0) + \Theta(n) = T(n-1) + \Theta(n).$

Este caso se da, por ejemplo, si el array ya está ordenado y siempre elegimos como pivote el último elemento (que sería el mayor). La partición no deja nada a un lado y $n-1$ elementos al otro. Esta recurrencia resuelta produce $T(n) = \Theta(n^2)$, porque:

- $T(n) = T(n-1) + cn$
    
- $T(n-1) = T(n-2) + c(n-1)$
    

sumando:

$T(n) = T(n-2) + c[(n) + (n-1)]$

eventualmente:

$T(n) = T(1) + c[ n + (n-1) + ... + 2 ] = \Theta(n^2)$.

En el **caso promedio**, con elección aleatoria de pivote o con datos no ordenados adversamente, Quick Sort suele producir divisiones razonablemente balanceadas en promedio. Se puede demostrar que el número esperado de comparaciones (o de operaciones) es $\Theta(n \log n)$. La forma rigurosa de obtener esto es un poco compleja, involucrando sumas de expectativas; pero intuitivamente, la **altura promedio del árbol de recursión** es $O(\log n)$. En cada nivel, en promedio, el pivote tiende a caer no siempre en la mitad exacta, pero tampoco cerca de los extremos la mayoría de las veces. El resultado conocido es que el caso promedio de Quick Sort es $\Theta(n \log n)$.

**Complejidad temporal:**

- **Mejor caso:** $O(n \log n)$. Sucede cuando cada elección de pivote divide la lista en partes casi iguales (balanceadas). Por ejemplo, si siempre el pivote resulta ser el elemento medio. En ese escenario ideal, Quick Sort se comporta igual que Merge Sort, con complejidad $\Theta(n \log n)$.
    
- **Peor caso:** $O(n^2)$. Ocurre cuando los pivotes elegidos resultan muy desbalanceados en cada partición, por ejemplo, siempre el más grande o más pequeño. Un caso clásico es ordenar una lista que ya está ordenada si elegimos como pivote siempre el último elemento (o siempre el primero sin aleatorizar): cada partición genera un subproblema de tamaño $n-1$ y otro de tamaño $0$, provocando la recurrencia $T(n) = T(n-1) + \Theta(n)$, cuya solución es cuadrática.
    
- **Caso promedio:** $O(n \log n)$. Con pivote aleatorio o datos aleatorios, Quick Sort muy probablemente dividirá suficientemente bien la lista en la mayoría de los niveles, logrando tiempo de ejecución lineal-logarítmico. En promedio, Quick Sort es más rápido que Merge Sort debido a patrones de acceso a memoria y constantes menores, haciendo que sea frecuentemente el algoritmo de ordenamiento interno más utilizado.
    

En conclusión, Quick Sort tiene un rendimiento excelente en promedio ($\Theta(n \log n)$) y en el mejor caso, pero hay que tener cuidado con su peor caso $\Theta(n^2)$.

**Complejidad espacial:** Una de las ventajas de Quick Sort es que se realiza **en sitio (in-place)**: no requiere estructuras auxiliares de tamaño $O(n)$ como Merge Sort. El particionado se hace mediante intercambios dentro del mismo array. Por lo tanto, el espacio extra necesario aparte del array de entrada es mínimo (constante para algunos índices temporales). Sin embargo, la **recursión** sí consume espacio en la pila: la profundidad de recursión en el mejor caso (y promedio) es $O(\log n)$, pero en el peor caso puede ser $O(n)$ (si el pivote deja $n-1$ elementos en un lado cada vez, las llamadas anidadas forman una cadena larga).

En la implementación típica, la complejidad espacial se suele dar como:

- **Promedio:** $O(\log n)$ de espacio auxiliar (por la pila de recursión).
    
- **Peor caso:** $O(n)$ de espacio auxiliar, en caso de una partición extremadamente desbalanceada repetida.
    

No obstante, existen optimizaciones para limitar la profundidad de recursion. Una estrategia común es **recurser primero en la sublista más pequeña** y usar recursividad de cola para la más grande; esto asegura que la pila de llamadas crezca $\log n$ incluso en casos desbalanceados. Con tales técnicas, Quick Sort se puede implementar para usar espacio $O(\log n)$ en el peor caso. En resumen, Quick Sort **no requiere espacio adicional significativo** más allá de la recursión.

## Torres de Hanói (_Tower of Hanoi_)

**Descripción del algoritmo:** El problema de las **Torres de Hanói** es un rompecabezas matemático clásico que se resuelve elegantemente con recursión. Se tienen tres varillas o postes y $n$ discos de diferentes tamaños apilados en una de las varillas en orden decreciente de tamaño (el más grande abajo, el más pequeño arriba). El objetivo es trasladar toda la pila de $n$ discos desde la varilla origen hasta otra varilla destino, utilizando la tercera varilla como auxiliar, respetando las siguientes reglas:

1. Solo se puede mover **un disco a la vez**.
    
2. Un disco **siempre** debe colocarse sobre otro disco más grande (nunca sobre uno más pequeño).
    
3. Solo se puede mover el disco superior de cualquiera de las pilas en un movimiento.
    

La solución recursiva directa: para mover $n$ discos de $A$ (origen) a $C$ (destino) usando $B$ (auxiliar):

- Mueve recursivamente $n-1$ discos de $A$ a $B$, usando $C$ como auxiliar.
    
- Mueve el disco restante (el más grande, disco $n$) de $A$ a $C$.
    
- Mueve recursivamente los $n-1$discos de $B$ (que ahora están en $B$) a $C$, usando $A$ como auxiliar.
    

Esta solución recursiva minimiza el número de movimientos. Resulta que el número mínimo de movimientos necesarios es $2^n - 1$ para $n$ discos, que crece exponencialmente con $n$.

**Código en Java (Torres de Hanói recursivo):**

```Java
public class TorresDeHanoi {

    public static void resolverHanoi(int n, char origen, char destino, char auxiliar) {
        if (n == 1) {
            System.out.println("Mover disco 1 de " + origen + " a " + destino);
        } else {
            // Mover n-1 discos de origen a auxiliar, usando destino como auxiliar
            resolverHanoi(n - 1, origen, auxiliar, destino);
            // Mover el disco n (más grande) de origen a destino
            System.out.println("Mover disco " + n + " de " + origen + " a " + destino);
            // Mover los n-1 discos del auxiliar al destino, usando origen como auxiliar
            resolverHanoi(n - 1, auxiliar, destino, origen);
        }
    }

    public static void main(String[] args) {
        int n = 3; // número de discos
        resolverHanoi(n, 'A', 'C', 'B'); // mover de A (origen) a C (destino) usando B (auxiliar)
    }
}
```


Por ejemplo, para $n = 3$ discos, la salida de este programa sería una secuencia de $7$ movimientos, que es $2^3 - 1$.

**Recurrencia:** Si denotamos por $T(n)$ el número mínimo de movimientos (o equivalente, el tiempo de ejecución) para resolver el problema con $n$ discos, la relación de recurrencia viene dada por:

$T(1) = 1,$

ya que con $1$ solo disco, solo se requiere un movimiento (mover ese disco directamente a la varilla destino).

Para $n > 1$, siguiendo la solución recursiva explicada:

1. Mover $n-1$ discos de la varilla origen a la auxiliar requiere $T(n-1)$ movimientos.
    
2. Luego mover el disco $n$ (el más grande) de origen a destino requiere **1** movimiento.
    
3. Finalmente, mover los $n-1$ discos de la varilla auxiliar a la destino requiere otros $T(n-1)$ movimientos.
    

Por lo tanto, la recurrencia es:

$T(n) = 2T(n-1) + 1$, para $n \ge 2$,

con $T(1) = 1$.

**Resolución de la recurrencia:**

- **Por expansión:**  
    $T(n) = 2T(n-1) + 1$  
    $T(n) = 2(2T(n-2) + 1) + 1 = 2^2 T(n-2) + 2^1 + 1$  
    $T(n) = 2^2(2T(n-3) + 1) + 2^1 + 1 = 2^3 T(n-3) + 2^2 + 2^1 + 1$
    
    Se puede conjeturar el patrón: después de $k$ expansiones,  
    $T(n) = 2^k T(n-k) + 2^{k-1} + 2^{k-2} + \dots + 2^1 + 1$.
    
    Si tomamos $k = n-1$ llegamos a $T(1)$:  
    $T(n) = 2^{n-1} T(1) + (2^{n-2} + 2^{n-3} + \dots + 2 + 1)$.
    
    Sabemos $T(1) = 1$, entonces:  
    $T(n) = 2^{n-1} \cdot 1 + \sum_{i=0}^{n-2} 2^i$.
    
    La sumatoria $\sum_{i=0}^{n-2} 2^i = 2^{n-1} - 1$ (es una serie geométrica). Por tanto:
    
    $T(n) = 2^{n-1} + (2^{n-1} - 1) = 2^n - 1.$
    

**Complejidad temporal:**

- **Mejor caso:** En este problema no hay mucha variación: cualquier instancia de $n$ discos **requiere al menos** $2^n - 1$ movimientos para resolverse (ese es el número mínimo demostrable de pasos).
    
- **Peor caso:** $O(2^n)$. Realmente, cada instancia de tamaño $n$ es el caso "peor" en cuanto a complejidad, ya que no hay entradas más difíciles o fáciles: todos requieren todos los movimientos. La complejidad temporal es exponencial $\Theta(2^n)$.
    
- **Caso promedio:** Similarmente, no tiene sentido hablar de promedio sobre distintas permutaciones de datos, puesto que el problema está definido únicamente por $n$.
    

En consecuencia, las Torres de Hanói son un ejemplo de un algoritmo con **tiempo exponencial**, que escala muy mal a medida que crece $n$.

**Complejidad espacial:** El algoritmo recursivo de Torres de Hanói no utiliza estructuras de datos auxiliares más allá de unas variables temporales para representar los postes y el contador de discos. Sin embargo, la **profundidad de la recursión** es $n$. La secuencia recursiva va moviendo $n-1$ discos, luego otros $n-1$, etc., pero en términos de llamadas anidadas, la llamada más profunda ocurre cuando llegamos al caso base moviendo un disco. Esto genera un árbol recursivo de profundidad $n$. Por lo tanto, la pila de llamadas consume espacio $O(n)$. Aparte de eso, no se requiere espacio adicional. Entonces, la complejidad espacial es **$O(n)$** (lineal en el número de discos).

## Potencia Recursiva (_Recursive Power Function_)

**Descripción del algoritmo:** Calcular la potencia $a^b$ (es decir, $a$ elevado a la $b$-ésima potencia) es un problema básico donde la recursión puede emplearse de distintas maneras. La definición directa es:

- $a^0 = 1$ (cualquier número a la potencia $0$ es $1$),
    
- $a^b = a \times a^{b-1}$ para $b \ge 1$.
    

Siguiendo esta definición, podemos implementar una función recursiva que multiplique $a$ consigo mismo $b$ veces. Este enfoque sencillo realiza $b$ multiplicaciones y tiene complejidad lineal en $b$. Sin embargo, existe un método recursivo más eficiente: usando **exponenciación binaria (divide and conquer)**. La idea es:

- Si $b$ es par, entonces $a^b = (a^{b/2}) \times (a^{b/2})$. Podemos calcular $a^{b/2}$ recursivamente y luego cuadrarlo.
    
- Si $b$ es impar, entonces $a^b = a \times a^{b-1}$, y $b-1$ es par. Aplicamos el caso anterior después de una multiplicación inicial.
    

Este método reduce drásticamente el número de multiplicaciones, de $b$ (lineal) a aproximadamente $\log_2 b$ (logarítmico).

**Código en Java (potencia recursiva simple vs. divide y vencerás):**

```Java
public class PotenciaRecursiva {

    // Método recursivo simple: multiplica a por sí mismo b veces (O(b) tiempo)
    public static long potenciaNaive(long a, int b) {
        if (b == 0) {
            return 1; // caso base: a^0 = 1
        }
        // caso recursivo: a^b = a * a^(b-1)
        return a * potenciaNaive(a, b - 1);
    }

    // Método recursivo optimizado: exponenciación rápida (O(log b) tiempo)
    public static long potenciaRapida(long a, int b) {
        if (b == 0) {
            return 1; // caso base
        }
        if (b % 2 == 0) {
            // b es par: calcular a^(b/2) una vez y multiplicarlo consigo mismo
            long mitad = potenciaRapida(a, b / 2);
            return mitad * mitad;
        } else {
            // b es impar: extraer una 'a', y reducir a^(b-1) que es par
            return a * potenciaRapida(a, b - 1);
        }
    }

    public static void main(String[] args) {
        long base = 3;
        int exponente = 10;
        System.out.println("Potencia naive: " + potenciaNaive(base, exponente));   // 3^10
        System.out.println("Potencia rápida: " + potenciaRapida(base, exponente)); // 3^10
    }
}
```


En este código, `potenciaNaive(a, b)` calcula $a^b$ de forma lineal, simplemente multiplicando recursivamente. Por otro lado, `potenciaRapida(a, b)` utiliza la técnica mencionada:

- Si $b$ es par, primero calcula recursivamente $a^{b/2}$ (reduciendo el problema a la mitad) y luego lo multiplica consigo mismo para obtener $a^b$. Solo $1$ multiplicación extra es necesaria (además de las que ocurren en la recursión).
    
- Si $b$ es impar, extrae un factor $a$ (eso cubre una multiplicación), y convierte el problema en $a^{b-1}$ que es par, resolviéndolo recursivamente.
    

**Recurrencias:**

- Para el método **naive**: Sea $T_1(b)$ el tiempo de `potenciaNaive(a, b)$. Tenemos:
    
    $T_1(b) = T_1(b-1) + O(1),$
    
    con $T_1(0) = O(1)$. Esta recurrencia es similar a la del factorial: su solución es $T_1(b) = O(b)$.
    
- Para el método **rápido**: Sea $T_2(b)$ el tiempo de `potenciaRapida(a, b)`. En el peor caso, alternará entre par e impar. Simplificando, notemos que aproximadamente la mitad de las veces $b$ se reduce a $b/2$ (caso par), y la otra mitad se reduce a $b-1$ (caso impar, luego inmediatamente a $(b-1)/2$ en la siguiente llamada). En el análisis asintótico, domina la división por $2$. Formalmente:
    
    - Si $b$ es par: $T_2(b) = T_2(b/2) + O(1)$.
        
    - Si $b$ es impar: $T_2(b) = T_2(b-1) + O(1)$, pero luego $b-1$ es par, así que en la siguiente recursión será $T_2((b-1)/2)$.
        
    
    En esencia, $T_2(b)$ cumple $T_2(b) = T_2(\lfloor b/2 \rfloor) + O(1)$. Esta es una recurrencia similar a la de la búsqueda binaria. Su solución es $T_2(b) = O(\log b)$.
    

**Resolución de las recurrencias:**

- Método _naive_: $T_1(b) = T_1(b-1) + C$. Siguiendo la expansión: $T_1(b) = T_1(b-1) + C = T_1(b-2) + 2C = ... = T_1(0) + bC = \Theta(b)$. Lineal en $b$.
    
- Método _rápido_: $T_2(b) = T_2(b/2) + C$ (aproximadamente). Expandiendo: $T_2(b) = T_2(b/2) + C = T_2(b/4) + 2C = ... = T_2(1) + C \log_2 b = \Theta(\log b)$.
    

**Complejidad temporal:**

- **Potenciación recursiva simple (naive):**
    
    - Mejor caso: $O(1)$ si $b = 0$.
        
    - Peor caso: $O(b)$, porque se realizan $b$ multiplicaciones recursivamente.
        
    - Caso promedio: $O(b)$, ya que para cualquier exponente no trivial la ejecución es lineal en $b$.
        
- **Potenciación recursiva rápida (divide y vencerás):**
    
    - Mejor caso: $O(1)$ si $b = 0$.
        
    - Peor caso: $O(\log b)$. Incluso en el patrón más desfavorable de impares, cada dos llamadas se reduce el exponente al menos a la mitad.
        
    - Caso promedio: $O(\log b)$. Para exponentes típicos, el algoritmo recorrerá los bits de $b$ (representación binaria), por lo que su complejidad es proporcional a la cantidad de bits, que es $\approx \log_2 b$.
        

En resumen, la versión recursiva optimizada logra **tiempo logarítmico** en $b$, mientras que la versión simple es **tiempo lineal** en $b$.

**Complejidad espacial:** Para ambos métodos, la recursividad implica una profundidad de pila proporcional al número de llamadas:

- En el método naive, la profundidad de recursión es $b$ (en el peor caso), por lo que el espacio auxiliar es $O(b)$.
    
- En el método rápido, la profundidad de recursión es $O(\log b)$ (ya que cada nivel reduce significativamente $b$), por lo que el espacio es $O(\log b)$.
    

Ninguno de los métodos utiliza estructuras auxiliares grandes aparte de la recursión.

## Búsqueda Lineal Recursiva

**Descripción del algoritmo:** La **búsqueda lineal** consiste en buscar un elemento en un arreglo o lista comprobando secuencialmente cada elemento hasta encontrarlo (o agotar la lista). Se puede implementar iterativamente con un bucle, o recursivamente haciendo que la función revise un elemento y luego llame a sí misma para el resto. No es un algoritmo especialmente eficiente ($O(n)$ en el peor caso), pero es sencillo y funciona en datos no ordenados.

**Código en Java (búsqueda lineal recursiva):**

```Java
public class BusquedaLinealRecursiva {

    // Busca 'objetivo' en el array 'arr' de forma recursiva.
    // Devuelve el índice si lo encuentra, o -1 si no está.
    public static int buscarRec(int[] arr, int objetivo, int indice) {
        if (indice >= arr.length) {
            return -1; // caso base: alcanzó el final sin encontrar
        }
        if (arr[indice] == objetivo) {
            return indice; // caso base: encontrado en 'indice'
        }
        // Llamada recursiva: buscar en el resto del array
        return buscarRec(arr, objetivo, indice + 1);
    }

    public static void main(String[] args) {
        int[] datos = {5, 8, 2, 9, 6};
        int objetivo = 9;
        int resultado = buscarRec(datos, objetivo, 0);
        if (resultado != -1) {
            System.out.println("Encontrado en índice: " + resultado);
        } else {
            System.out.println("No encontrado");
        }
    }
}
```


Explicación:

- `buscarRec(arr, objetivo, indice)` es la función recursiva que inspecciona el elemento en la posición `indice`.
    
- Si `indice` llega a ser igual al tamaño del array, significa que hemos revisado todos los elementos y no encontramos el objetivo (caso base de no encontrado).
    
- Si el elemento en `arr[indice]` es el buscado, retornamos `indice` (caso base de encontrado).
    
- Si no, llamamos recursivamente a `buscarRec` incrementando `indice` para revisar el siguiente elemento.
    

**Recurrencia:** Si $T(n)$ es el tiempo en el peor caso para buscar en un arreglo de $n$ elementos:

- En cada llamada, se realiza una comparación de costo constante $O(1)$.
    
- Luego, si no es caso base, se llama recursivamente con el siguiente índice, lo que esencialmente es buscar en los $n-1$ elementos restantes.
    

Por tanto, la recurrencia es:

$T(n) = T(n-1) + O(1),$

con $T(0) = O(1)$.

**Resolución de la recurrencia:** Por expansión:

$T(n) = T(n-1) + C = T(n-2) + 2C = ... = T(0) + nC = \Theta(n).$

**Complejidad temporal:**

- **Mejor caso:** $O(1)$. Sucede si el elemento buscado es el primero en la lista (índice $0$).
    
- **Peor caso:** $O(n)$. Ocurre si el elemento no está en el array, o está en la última posición.
    
- **Caso promedio:** $O(n)$. Si asumimos que la posición del elemento buscado es aleatoria, en promedio se recorrerá la mitad de la lista antes de encontrarlo (pero asintóticamente es $\Theta(n)$).
    

La búsqueda lineal recursiva tiene por tanto **complejidad lineal** $\Theta(n)$.

**Complejidad espacial:** La implementación recursiva tiene una profundidad de pila de hasta $n$ en el peor caso (si recorre todo el array recursivamente). Por tanto, usa $O(n)$ espacio adicional en la pila de llamadas. La búsqueda lineal iterativa tradicional tiene complejidad espacial $O(1)$ (no usa espacio extra, solo un índice de loop).

## Máximo de un Array (búsqueda del valor máximo de forma recursiva)

**Descripción del algoritmo:** Encontrar el **valor máximo** en un array de $n$ elementos es una tarea que inevitablemente requiere examinar cada elemento al menos una vez (para estar seguros de cuál es el mayor). Una solución recursiva simple es similar a la búsqueda lineal pero guardando el máximo: podemos comparar el primer elemento con el máximo del resto del array recursivamente. Alternativamente, se puede utilizar divide y vencerás: dividir el array en dos mitades, encontrar el máximo de cada mitad recursivamente, y luego tomar el mayor de esos dos resultados. Ambas aproximaciones tienen complejidad $\Theta(n)$, porque todos los elementos deben evaluarse al menos una vez.

**Código en Java (enfoque recursivo lineal para máximo):**

```Java
public class MaximoRecursivo {

    // Devuelve el valor máximo en el subarreglo arr[0..n-1] recursivamente
    public static int maxRec(int[] arr, int n) {
        if (n == 1) {
            return arr[0]; // caso base: un solo elemento, ese es el max
        }
        // Obtener el máximo del subarreglo arr[0..n-2] recursivamente
        int maxSub = maxRec(arr, n - 1);
        // Comparar el último elemento arr[n-1] con el máximo de la sublista
        if (arr[n - 1] > maxSub) {
            return arr[n - 1];
        } else {
            return maxSub;
        }
    }

    public static void main(String[] args) {
        int[] datos = {12, 3, 19, 4, 8, 7};
        int max = maxRec(datos, datos.length);
        System.out.println("El valor máximo es: " + max);
    }
}
```


Aquí, `maxRec(arr, n)` devuelve el máximo de los primeros `n` elementos de `arr`. El caso base es cuando $n = 1$, simplemente retorna `arr[0]`. Luego, recurre para $n-1$ elementos y compara el resultado con el elemento $n$-ésimo (último) para determinar el máximo global.

**Recurrencia:** Sea $T(n)$ el tiempo para encontrar el máximo de $n$ elementos:

$T(n) = T(n-1) + O(1),$

con $T(1) = O(1)$. Cada llamada recursiva reduce el problema en $1$ y realiza una comparación constante. Esta es la misma recurrencia lineal ya vista en varios ejemplos, cuya solución es $T(n) = \Theta(n)$.

Si en lugar de la estrategia lineal hubiéramos usado divide y vencerás (dividir en dos mitades de $\approx n/2$):

$T(n) = T(n/2) + T(n/2) + O(1),$

con $T(1) = O(1)$. Aquí $a = 2$, $b = 2$, $f(n) = O(1)$. Aplicando Teorema Maestro, $n^{\log_b a} = n^1$, y $f(n)$ es más pequeño ($O(n^c)$ con $c = 0 < 1$), así que es caso 1: $T(n) = \Theta(n)$ también.

**Complejidad temporal:**

- **Mejor caso:** $O(n)$ en general. Incluso si el primer elemento fuera el mayor, un algoritmo correcto de máximo todavía tiene que verificar los demás elementos para asegurarse.
    
- **Peor caso:** $O(n)$. Hay que revisar todos los elementos.
    
- **Caso promedio:** $O(n)$. En todos los casos no triviales se recorren todos los elementos.
    

La tarea de hallar el máximo es inherentemente $\Omega(n)$ (al menos $n-1$ comparaciones son necesarias), por lo que nuestro algoritmo es óptimo en orden de crecimiento.

**Complejidad espacial:** En la implementación dada (lineal recursiva), la profundidad de recursión es $n$, consumiendo $O(n)$ en la pila. Si hubiésemos usado la estrategia de dividir a la mitad, la profundidad sería $O(\log n)$, usando menos espacio. Iterativamente sería $O(1)$. Resumiendo:

- Espacio recursivo (enfoque lineal): $\Theta(n)$ en peor caso.
    
- Espacio recursivo (enfoque dividir y vencerás): $\Theta(\log n)$ en peor caso.
    
- Espacio iterativo: $\Theta(1)$.
    

---

Todas estas conclusiones ilustran cómo las **relaciones de recurrencia** nos ayudan a predecir y explicar el comportamiento asintótico de algoritmos recursivos, reforzando nuestro entendimiento de su eficiencia. Hemos cubierto desde ejemplos sencillos hasta algoritmos de ordenación fundamentales, proporcionando una visión completa de técnicas de análisis de recurrencias. Cada método tiene sus fortalezas y debilidades en términos de tiempo y espacio, y el uso apropiado de la recursión (junto con optimizaciones como la memoización o la elección astuta de pivotes) puede marcar la diferencia entre un algoritmo impracticable y uno óptimo.